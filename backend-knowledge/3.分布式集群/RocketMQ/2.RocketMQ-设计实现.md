# RocketMQ


[toc]

## 架构设计

整体架构，生产者发送消息，Broker存储消息，消费者消费消息；

通信机制，顺序写/PageCache，零拷贝，消费负载均衡，高可用，事务消息，延时消息

### 1、技术架构

![rocketmq_architecture_1.png](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_architecture_1.png)

RocketMQ 架构上主要分为四部分，如上图所示:

- Producer：消息发布的角色，支持分布式集群方式部署。Producer 通过 MQ 的负载均衡模块选择相应的 Broker 集群队列进行消息投递，投递的过程支持快速失败并且低延迟。

- Consumer：消息消费的角色，支持分布式集群方式部署。支持以 push 推，pull 拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。

- NameServer：NameServer 是一个非常简单的 Topic 路由注册中心，其角色类似 Dubbo 中的 zookeeper，支持 Broker 的动态注册与发现。主要包括两个功能：
    - Broker 管理，NameServer 接受 Broker 集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查 Broker 是否还存活；
    - 路由信息管理，每个 NameServer 将保存关于 Broker 集群的整个路由信息和用于客户端查询的队列信息。然后 Producer 和 Conumser 通过 NameServer 就可以知道整个 Broker 集群的路由信息，从而进行消息的投递和消费。NameServer 通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker 是向每一台 NameServer 注册自己的路由信息，所以每一个 NameServer 实例上面都保存一份完整的路由信息。当某个 NameServer 因某种原因下线了，Broker 仍然可以向其它 NameServer 同步其路由信息，Producer, Consumer 仍然可以动态感知 Broker 的路由的信息。 

- BrokerServer：Broker 主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker 包含了以下几个重要子模块。
    - Remoting Module：整个 Broker 的实体，负责处理来自 clients 端的请求。
    - Client Manager：负责管理客户端(Producer/Consumer)和维护 Consumer 的 Topic 订阅信息
    - Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。
    - HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。
    - Index Service：根据特定的Message key 对投递到 Broker 的消息进行索引服务，以提供消息的快速查询。

![rocketmq_architecture_2](https://gitee.com/bruceyum/pictures/raw/master/pics/rocketmq_architecture_2.png)

### 2、部署架构

![rocketmq 部署架构](https://gitee.com/bruceyum/pictures/raw/master/pics/rocketmq_architecture_3.png)

RocketMQ 网络部署特点

- NameServer 是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。

- Broker 部署相对复杂，Broker 分为 Master 与 Slave，一个 Master 可以对应多个 Slave，但是一个 Slave 只能对应一个 Master，Master 与 Slave 的对应关系通过指定相同的 BrokerName，不同的 BrokerId 来定义，BrokerId 为 0 表示 Master，非 0 表示 Slave。Master 也可以部署多个。每个 Broker 与 NameServer 集群中的所有节点建立长连接，定时注册 Topic 信息到所有 NameServer。 注意：当前 RocketMQ 版本在部署架构上支持一 Master 多 Slave，但只有 BrokerId=1 的从服务器才会参与消息的读负载。

- Producer 与 NameServer 集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Master 建立长连接，且定时向 Master 发送心跳。Producer 完全无状态，可集群部署。

- Consumer 与 NameServer 集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Master、Slave 建立长连接，且定时向 Master、Slave 发送心跳。Consumer 既可以从 Master 订阅消息，也可以从 Slave 订阅消息，消费者在向 Master 拉取消息时，Master 服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读 I/O），以及从服务器是否可读等因素建议下一次是从 Master 还是 Slave 拉取。

结合部署架构图，描述集群工作流程：

- 启动 NameServer，NameServer 起来后监听端口，等待 Broker、Producer、Consumer 连上来，相当于一个路由控制中心。
- Broker 启动，跟所有的 NameServer 保持长连接，定时发送心跳包。心跳包中包含当前 Broker 信息( IP+ 端口等)以及存储所有 Topic 信息。注册成功后，NameServer 集群中就有 Topic 跟 Broker 的映射关系。
- 收发消息前，先创建 Topic，创建 Topic 时需要指定该 Topic 要存储在哪些 Broker 上，也可以在发送消息时自动创建 Topic。
- Producer 发送消息，启动时先跟 NameServer 集群中的其中一台建立长连接，并从 NameServer 中获取当前发送的 Topic 存在哪些 Broker 上，轮询从队列列表中选择一个队列，然后与队列所在的 Broker 建立长连接从而向 Broker 发消息。
- Consumer 跟 Producer 类似，跟其中一台 NameServer 建立长连接，获取当前订阅 Topic 存在哪些 Broker 上，然后直接跟 Broker 建立连接通道，开始消费消息。


## NameServer

NameServer 是整个消息队列中的状态服务器，集群的各个组件通过它来了解全局的信息。同时，各个角色的机器都要定期向 NameServer 上报自己的状态，超时不上报的话，NameServer 会认为某个机器出故障不可用了，其他的组件会把这个机器从可用列表里移除。

### 1、集群状态维护

topicQueueTable、BrokerAddrTable、ClusterAddrTable、brokerLiveInfo、FilterServer

HashMap<String topic, List<QueueData>> topicQueueTable：Key 是 Topic 的名称，它存储了所有 Topic 的属性信息。Value 是个 QueueData 队列，队里的长度等于这个 Topic 数据存储的 Master Broker 的个数，QueueData 里存储着 Broker 的名称、读写 queue 的数量、同步标识等。

HashMap<String BrokerName, BrokerData> BrokerAddrTable：这个结构存储着一个 BrokerName 对应的属性信息，包括所属的 Cluster 名称，一个 Master Broker 和多个 Slave Broker 的地址信息

HashMap<String ClusterName, Set<String BrokerName>> ClusterAddrTable：存储的是集群中 Cluster 的信息，结果很简单，就是一个 Cluster 名称对应一个由 BrokerName 组成的集合。

HashMap<String BrokerAddr, BrokerLiveInfo> Broker-LiveTable：Key 是 BrokerAddr 对应着一台机器，BrokerLiveTable 存储的内容是这台 Broker 机器的实时状态，包括上次更新状态的时间戳，NameServer 会定期检查这个时间戳，超时没有更新就认为这个 Broker 无效了，将其从 Broker 列表里清除。

HashMap<String BrokerAddr, List<String> FilterServer> filterServerTable：Key 是 Broker 的地址，Value 是和这个 Broker 关联的多个 FilterServer 的地址。Filter Server 是过滤服务器，是 RocketMQ 的一种服务端过滤方式，一个 Broker 可以有一个或多个 Filter Server。

其他角色会主动向 NameServer 上报状态，所以 NameServer 的主要逻辑在 DefaultRequestProcessor 类中，根据上报消息里的请求码做相应的处理，更新存储的对应信息。

- Broker 接到创建 Topic 的请求后向 NameServer 发送注册信息，NameServer 收到注册信息后首先更新Broker信息，然后对每个 Master 角色的 Broker，创建一个 QueueData 对象。如果是新建 Topic，就是添加 QueueData 对象；如果是修改 Topic，就是把旧的 QueueData 删除，加入新的 QueueData。

- Broker 向 NameServer 发送的心跳会更新时间戳，NameServer 每 10 秒检查一次检查时间戳，检查到时间戳超过 2 分钟则认为 Broker 已失效，便会触发清理逻辑。

- 连接断开的事件也会触发状态更新，当 NameServer 和 Broker 的长连接断掉以后，onChannelDestroy 函数会被调用，把这个 Broker 的信息清理出去。

### 2、NameServer 解析

NameServer是集群的协调者，它只是简单地接收其他角色报上来的状态，然后根据请求返回相应的状态。NamesrvStartup 是模块的启动入口，NamesrvController 是用来协块各个调模功能的代码。

**1. NamesrvStartup**

解析命令行参数：-c命令行参数用来指定配置文件的位置；-p命令行参数用来打印所有配置项的值。注意，用-p参数打印配置项的值之后程序就退出了，这是一个帮助调试的选项。

初始化 NameServer 的 Controller：根据解析出的配置参数，调用 controller.initialize() 来初始化，然后调用 controller.start() 让 NameServer 开始服务。

还有一个逻辑是注册 ShutdownHookThread，当程序退出的时候会调用 controller.shutdown 来做退出前的清理工作。

**2. NamesrvController**

首先，NameserverController把执行线程池初始化好，启动了一个默认是 8 个线程的线程池，还有两个定时执行的线程，一个用来扫描失效的 Broker（scanNotActiveBroker），另一个用来打印配置信息（printAllPeriodically）。

然后启动负责通信的服务remotingServer, remotingServer监听一些端口，收到Broker、Client等发过来的请求后，根据请求的命令，调用不同的Processor来处理。

NameServer的核心业务逻辑，在DefaultRequestProcessor中：根据RequestCode调用不同的函数来处理，从RequestCode可以了解到NameServer的主要功能，比如：REGISTER_BROKER是在集群中新加入一个Broker机器；GET_ROUTEINTO_BY_TOPIC是请求获取一个Topic的路由信息；WIPE_WRITE_PERM_OF_BROKER是删除一个Broker的写权限。

### 3、为何不用ZooKeeper

ZooKeeper 是 Apache 的一个开源软件，为分布式应用程序提供协调服务。那为什么 RocketMQ 要自己造轮子，开发集群的管理程序呢？答案是 ZooKeeper 的功能很强大，包括自动 Master 选举等，RocketMQ 的架构设计决定了它不需要进行 Master 选举，用不到这些复杂的功能，只需要一个轻量级的元数据服务器就足够了。

中间件对稳定性要求很高，RocketMQ 的 NameServer 只有很少的代码，容易维护，所以不需要再依赖另一个中间件，从而减少整体维护成本。

## 消息发送

DefaultMQProducer 用于生产普通消息、顺序消息、单向消息、批量消息、延迟消息;

TransactionMQProducer 用于生产事务消息;

### 1、生产者启动


生产者每 30s 会从某台 NameServer 获取 Topic 和 Broker 的映射关系存在本地内存中，如果发现新的 Broker 就会和其建立长连接，每 30s 会发送心跳至 Broker 维护连接。

并且会轮询当前可以发送的 Broker 来发送消息，达到负载均衡的目的。

![producer.start()](https://gitee.com/bruceyum/pictures/raw/master/pics/70fc1229102eb19a9f7b1f408c2b3ff0)

1. 检查配置是否合法；
2. 启动客户端 Netty，启动后，可以向外部发送请求；
3. 创建更新 topic 信息的定时任务；
4. 创建向 Broker 发送心跳的定时任务，同时清理无效 Broker。

RemotingClient 是一个接口类，底层使用的通讯框架是Netty，提供了实现类 NettyRemotingClient，RemotingClient 在初始化的时候实例化 Bootstrap，方便后续用来创建 Channel；

总共启动了5个定时器任务，分别是：定时更新 NameServerAddr 信息，定时更新 topic 的路由信息，定时清理下线的 broker，定时持久化 Consumer 的 Offset 信息，定时调整线程池；

因为 Producer 和 Consumer 都需要用 MQClientInstance，MQClientInstance 实例的功能是管理本实例中全部生产者与消费者的生产和消费行为。同一个 clientId 是共用一个 MQClientInstance 的， clientId 是通过本机 IP 和 instanceName（默认值 default）拼起来的。消息拉取服务 pullMessageService 和 重平衡服务 rebalanceService 被用在消费端的两个服务类，分别是：从broker拉取消息的服务和均衡消息队列服务，负责分配消费者可消费的消息队列；

源码解析：

```java
// 这是启动整个生产者实例的入口，主要负责校验生产者的配置参数是否正确，
// 并启动通信通道、各种定时计划任务、Pull服务、Rebalance服务、注册生产者到Broker等操作。
public void start(final boolean startFactory) throws MQClientException {
    switch (this.serviceState) {
        // 通过 switch-case 判断当前生产者的服务状态，创建时默认状态是CREATE_JUST。
        case CREATE_JUST:
            // 设置默认启动状态为启动失败。
            // 校验生产者实例设置的各种参数。比如生产者组名是否为空、是否满足命名规则、长度是否满足等。
            // 校验生产者组名，如果是默认名字则将其修改为进程id。
            // 根据clientId@instantName获取或者初始化一个MQClientInstance
            this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook);
            //缓存 Topic 对应的 路由信息。此时路由信息还是空，会在发送消息时或者定时从nameServer获取路由信息
            this.topicPublishInfoTable.put(this.defaultMQProducer.getCreateTopicKey(), new TopicPublishInfo());
            //如果要启动MQClientInstant
            if (startFactory) {
                mQClientFactory.start();
            }
        default:
            break;
    }
    //向所有Brokers发送心跳
    this.mQClientFactory.sendHeartbeatToAllBrokerWithLock();
}
public void start() throws MQClientException {

    synchronized (this) {
        switch (this.serviceState) {
            case CREATE_JUST:
                this.serviceState = ServiceState.START_FAILED;
                // If not specified,looking address from name server
                if (null == this.clientConfig.getNamesrvAddr()) {
                    this.mQClientAPIImpl.fetchNameServerAddr();
                }
                // 初始化 Netty 远程客户端 RemotingClient 
                this.mQClientAPIImpl.start();
                // Start various schedule tasks
                this.startScheduledTask();
                // Start pull service
                this.pullMessageService.start();
                // Start rebalance service
                this.rebalanceService.start();
                // Start push service MARK 为什么这里要启动Producer???
                this.defaultMQProducer.getDefaultMQProducerImpl().start(false);
                log.info("the client factory [{}] start OK", this.clientId);
                this.serviceState = ServiceState.RUNNING;
                break;
            default:
                break;
        }
    }
}
public void MQClientAPIImpl#start() {
    this.remotingClient.start();
}
public void NettyRemotingClient#start() {
    Bootstrap handler = this.bootstrap.group(this.eventLoopGroupWorker).channel(NioSocketChannel.class)
}
```



### 2、消息发送流程

总体上讲，消息发送流程首先是 RocketMQ 客户端接收业务层消息，然后通过 DefaultMQProducerImpl 发送一个 RPC 请求给 Broker，再由 Broker 处理请求并保存消息。

业务层：通常指直接调用RocketMQ Client发送API的业务代码。
消息处理层：指 RocketMQ Client 获取业务发送的消息对象后，一系列的参数检查、消息发送准备、参数包装等操作。
通信层：指 RocketMQ 基于 Netty 封装的一个 RPC 通信服务，RocketMQ 的各个组件之间的通信全部使用该通信层。

![消息发送](https://gitee.com/bruceyum/pictures/raw/master/pics/bab32a227a1ecb69a73805c00b96561f)

1. 检查消息是否为空，消息的Topic的名字是否为空或者是否符合规范；消息体大小是否符合要求，最大值为4MB，可以通过 maxMessageSize 进行设置。
2. 执行tryToFindTopicPublishInfo()方法：获取Topic路由信息，如果不存在则发出异常提醒用户。如果本地缓存没有路由信息，就通过Namesrv获取路由信息，更新到本地，如果还是获取不到则使用默认的 topic 名称为"TBW102"去获取路由信息。
3. 第三步，计算消息发送的重试次数，同步重试和异步重试的执行方式是不同的。在同步发送情况下如果发送失败会默认重投两次（retryTimesWhenSendFailed = 2），并且不会选择上次失败的 broker，会向其他 broker 投递。在异步发送失败的情况下也会重试，默认也是两次 （retryTimesWhenSendAsyncFailed = 2），但是仅在同一个 Broker 上重试。
4. 第四步，执行队列选择方法selectOneMessageQueue（）。根据队列对象中保存的上次发送消息的Broker的名字和Topic路由，选择（轮询）一个Queue将消息发送到Broker。我们可以通过sendLatencyFaultEnable 来设置是否总是发送到延迟级别较低的Broker，默认值为False。
5. 第五步，执行sendKernelImpl（）方法。该方法是发送消息的核心方法，主要用于准备通信层的入参（比如Broker地址、请求体等），将请求传递给通信层，内部实现是基于Netty的，在封装为通信层request对象RemotingCommand前，会设置RequestCode表示当前请求是发送单个消息还是批量消息。

TBW102 就是接受自动创建主题的， Broker 启动会把这个默认主题登记到 NameServer，这样当 Producer 发送新 Topic 的消息时候就得知哪个 Broker 可以自动创建主题，然后发往那个 Broker。 Broker 接受到这个消息的时候发现没找到对应的主题，但是它接受创建新主题，这样就会创建对应的 Topic 路由信息。

成功获取到路由信息之后，需要调用 MQFaultStrategy 从MessageQueue列表中选择一个 MessageQueue（第一次随机，之后轮询）,然后检查是否超时，消息长度，封装消息头，调用RemotiojCommond发送消息。

再来看一下消息发送相关源码：

```java
private SendResult sendDefaultImpl(
        Message msg,
        final CommunicationMode communicationMode,
        final SendCallback sendCallback,
        final long timeout
    ) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
    // 第一步，两个检查：生产者状态、消息及消息内容。
    // 没有运行的生产者不能发送消息。消息检查主要检查消息是否为空，
    // 消息的Topic的名字是否为空或者是否符合规范；消息体大小是否符合要求，最大值为4MB，
    // 可以通过maxMessageSize进行设置。
    this.makeSureStateOK();
    // 执行tryToFindTopicPublishInfo（）方法：获取Topic路由信息，如果不存在则发出异常提醒用户。
    // 如果本地缓存没有路由信息，就通过Namesrv获取路由信息，更新到本地，再返回。
    TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic());
    // 第三步，计算消息发送的重试次数，同步重试和异步重试的执行方式是不同的。
    int timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1;
    for (; times < timesTotal; times++) {
        String lastBrokerName = null == mq ? null : mq.getBrokerName();
        // 执行队列选择方法selectOneMessageQueue（）。
        // 根据队列对象中保存的上次发送消息的Broker的名字和Topic路由，
        // 选择（轮询）一个Queue将消息发送到Broker
        MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName);
        // 第五步，执行sendKernelImpl（）方法。该方法是发送消息的核心方法，
        // 主要用于准备通信层的入参（比如Broker地址、请求体等），将请求传递给通信层，
        // 内部实现是基于Netty的，在封装为通信层request对象RemotingCommand前，
        // 会设置RequestCode表示当前请求是发送单个消息还是批量消息。
        sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime);
    }
}
 private SendResult sendKernelImpl(final Message msg,// 待发送消息。
             final MessageQueue mq,// 消息将发送到该消息队列上。
             final CommunicationMode communicationMode,// 消息发送模式，SYNC、ASYNC、ONEWAY
             final SendCallback sendCallback,// 异步消息回调函数。
             final TopicPublishInfo topicPublishInfo,// 主题路由信息
             final long timeout/*消息发送超时时间。*/) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
     //根据MessageQueue获取Broker的网络地址。如果MQClientInstance的brokerAddrTable未缓存该Broker的信息，则从NameServer主动更新一下topic的路由信息。
     // 如果路由更新后还是找不到Broker信息，则抛出MQClientException，提示Broker不存在。
     String brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());
     if (null == brokerAddr) {
         tryToFindTopicPublishInfo(mq.getTopic());
         brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());
     }
     // 如果消息体默认超过4K（compressMsgBodyOverHowmuch），会对消息体采用zip压缩，
     // 并设置消息的系统标记为MessageSysFlag.COMPRESSED_FLAG
     // 如果是事务Prepared消息，则设置消息的系统标记为MessageSysFlag.TRANSACTION_PREPARED_TYPE
     //如果注册了消息发送钩子函数，则执行消息发送之前的增强逻辑。
     // 通过DefaultMQProducerImpl#registerSendMessageHook注册钩子处理类，并且可以注册多个
     // 建消息发送请求包。主要包含如下重要信息：生产者组、主题名称、默认创建主题Key、
     // 该主题在单个Broker默认队列数、队列ID（队列序号）、消息系统标记（MessageSysFlag）、
     // 消息发送时间、消息标记（RocketMQ对消息中的flag不做任何处理，供应用程序使用）、
     //消息扩展属性、消息重试次数、是否是批量消息等。
     // 处理重试消息
     case SYNC:
         sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage(
             brokerAddr,
             mq.getBrokerName(),
             msg,
             requestHeader,
             timeout - costTimeSync,
             communicationMode,
             context,
             this);
         break;
 }
public SendResult MQClientAPIImpl#sendMessage(/*参数略*/){
    // 构建远程调用请求
    RemotingCommand request = null;
    request = RemotingCommand.createRequestCommand(RequestCode.SEND_MESSAGE, requestHeader);
    switch (communicationMode) {
        case SYNC:
            return this.sendMessageSync(addr, brokerName, msg, timeoutMillis - costTimeSync, request);
    }
}
private SendResult MQClientAPIImpl#sendMessageSync(/*参数略*/){
    // 远程调用
    RemotingCommand response = this.remotingClient.invokeSync(addr, request, timeoutMillis);
    assert response != null;
    return this.processSendResponse(brokerName, msg, response);
}
public RemotingCommand invokeSync(String addr, final RemotingCommand request, long timeoutMillis){
    // 获取或创建netty channel；
    final Channel channel = this.getAndCreateChannel(addr);
    // KEYPOINT 远程调用
    RemotingCommand response = this.invokeSyncImpl(channel, request, timeoutMillis - costTime);
}
public RemotingCommand invokeSyncImpl(final Channel channel, final RemotingCommand request,final long timeoutMillis){
    final ResponseFuture responseFuture = new ResponseFuture(channel, opaque, timeoutMillis, null, null);
    // RocketMQ每次发送同步请求前都会为一个request分配一个opaque，这是一个原子自增的id，
    // 一个response会以opaque作为key保存在responseTable中，这样用opaque就将request和response连接起来了。
    this.responseTable.put(opaque, responseFuture);
    final SocketAddress addr = channel.remoteAddress();
    //调用netty方法，发送请求并添加监听器
    channel.writeAndFlush(request).addListener(new ChannelFutureListener() {
        @Override
        public void operationComplete(ChannelFuture f) throws Exception {
            if (f.isSuccess()) {    //如果调用成功则返回
                responseFuture.setSendRequestOK(true);
                return;
            } else {
                responseFuture.setSendRequestOK(false);
            }

            responseTable.remove(opaque);
            responseFuture.setCause(f.cause());
            responseFuture.putResponse(null);
            log.warn("send a request command to channel <" + addr + "> failed.");
        }
    });
}
```




## 消息存储

消息存储是 RocketMQ 中最为复杂和最为重要的一部分，本节将分别从 RocketMQ 的消息存储整体架构、消息存储流程、PageCache 与 Mmap 内存映射以及 RocketMQ 中两种不同的刷盘方式来分别展开叙述。

### 1、存储架构

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_1.png)消息存储架构图中主要有下面三个跟消息存储相关的文件构成。

**1. CommitLog**

CommitLog：消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认 1G ，文件名长度为 20 位，左边补零，剩余为起始偏移量。<br>
比如 00000000000000000000 代表了第一个文件，起始偏移量为 0，文件大小为 1G=1073741824；当第一个文件写满了，第二个文件为 00000000001073741824，起始偏移量为 1073741824，以此类推。<br>
消息主要是顺序写入日志文件，当文件满了，写入下一个文件。<br>

- CommitLog顺序写，可以大大提高写入效率。
- 虽然是随机读，但是利用操作系统的 pagecache 机制，可以批量地从磁盘读取，作为 cache 存到内存中，加速后续的读取速度。

**2. ConsumeQueue**

ConsumeQueue：消息消费队列，本质是 CommitLog 的索引。RocketMQ 是基于主题 topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 topic 检索消息是非常低效的，ConsumeQueue 目的主要是提高消息消费的性能。<br>
ConsumeQueue（逻辑消费队列）作为消费消息的索引，consumequeue 文件采取定长设计，每一个条目共 20 个字节，分别为 8 字节的 commitlog 物理偏移量、4 字节的消息长度、8 字节 tag hashcode，单个文件由 30W 个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue 文件大小约 5.72M。ConsumeQueue 文件可以看成是基于 topic 的 commitlog 索引文件。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。
ConsumeQueue 文件夹的组织方式如下：topic/queue/file 三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。

为了保证完全的顺序写，需要 ConsumeQueue 这个中间结构，因为 ConsumeQueue 里只存偏移量信息，所以尺寸是有限的，在实际情况中，大部分的 ConsumeQueue 能够被全部读入内存，所以这个中间结构的操作速度很快，可以认为是内存读取的速度。此外为了保证 CommitLog 和 ConsumeQueue 的一致性，CommitLog 里存储了 ConsumeQueues、Message Key、Tag 等所有信息，即使 ConsumeQueue 丢失，也可以通过 CommitLog 完全恢复出来。

**3. IndexFile**

IndexFile：IndexFile（索引文件）提供了一种可以通过 key 或时间区间来查询消息的方法。IndexFile 的底层存储设计为在文件系统中实现 HashMap 结构，故 rocketmq 的索引文件其底层实现为 hash 索引。<br>
Index 文件的存储位置是：$HOME \store\index\${fileName}，文件名 fileName 是以创建时的时间戳命名的，固定的单个 IndexFile 文件大小约为 400M，一个 IndexFile 可以保存 2000W 个索引。<br>

在上面的 RocketMQ 的消息存储整体架构图中可以看出，RocketMQ 采用的是混合型的存储结构，即为 Broker 单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。RocketMQ 的混合型存储结构(多个 Topic 的消息实体内容都存储于一个 CommitLog 中)针对 Producer 和 Consumer 分别采用了数据和索引部分相分离的存储结构，Producer 发送消息至 Broker 端，然后 Broker 端使用同步或者异步的方式对消息刷盘持久化，保存至 CommitLog 中。只要消息被刷盘持久化至磁盘文件 CommitLog 中，那么 Producer 发送的消息就不会丢失。正因为如此，Consumer 也就肯定有机会去消费这条消息。当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，如果一个消息拉取请求未拉取到消息，Broker 允许等待 30s 的时间，只要这段时间内有新消息到达，将直接返回给消费端。这里，RocketMQ 的具体做法是，使用 Broker 端的后台服务线程—ReputMessageService 不停地分发请求并异步构建 ConsumeQueue（逻辑消费队列）和 IndexFile（索引文件）数据。

### 2、启动流程

Broker 是 RocketMQ 中核心的模块之一，主要负责处理各种 TCP 请求（计算）和存储消息（存储）。Broker 分为 Master 和 Slave。Master 主要提供服务，Slave 在 Master 宕机后提供消费服务。

![Broker 启动流程](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210721173109256.png)

启动命令分为两个脚本：./bin/mqbroker 和./bin/runbroker.sh。mqbroker 准备了RocketMQ 启动本身的环境数据，比如 ROCKETMQ_HOME 环境变量。runbroker.sh 主要设置了JVM启动参数，比如JAVA_HOME、Xms、Xmx。

第一步：初始化启动环境。这是由./bin/mqbroker 和./bin/runbroker.sh 两个脚本来完成的。/bin/mqbroker 脚本主要用于设置 RocketMQ 根目录环境变量，调用./bin/runbroker.sh 进入 RocketMQ 的启动入口

第二步：初始化BrokerController。该初始化主要包含RocketMQ启动命令行参数解析、Broker各个模块配置参数解析、Broker各个模块初始化、进程关机Hook初始化等过程。

第三步：启动RocketMQ的各个组件。

this.messageStore：存储层服务，比如CommitLog、ConsumeQueue存储管理。
this.remotingServer：普通通道请求处理服务。一般的请求都是在这里被处理的。
this.fastRemotingServer：VIP 通道请求处理服务。如果普通通道比较忙，那么可以使用VIP通道，一般作为客户端降级使用。
this.brokerOuterAPI：Broker访问对外接口的封装对象。
this.pullRequestHoldService：Pull长轮询服务。
this.clientHousekeepingService：清理心跳超时的生产者、消费者、过滤服务器。
this.filterServerManager：过滤服务器管理。

### 3、存储流程

RocketMQ 首先将消息数据写入操作系统 PageCache，然后定时将数据刷入磁盘。首先分析 RocketMQ 是如何接收发送消息请求并将消息写入 PageCache(MappedByteBuffer) 的。

![image-20210722142549838](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210722142549838.png)

接下来看一下消息存储相关源码：

```java
class NettyServerHandler extends SimpleChannelInboundHandler<RemotingCommand> {

    @Override
    protected void channelRead0(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception {
        processMessageReceived(ctx, msg);
    }
}
public void processMessageReceived(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception {
    final RemotingCommand cmd = msg;
    if (cmd != null) {
        switch (cmd.getType()) {
            case REQUEST_COMMAND:
                processRequestCommand(ctx, cmd);
                break;
            case RESPONSE_COMMAND:
                processResponseCommand(ctx, cmd);
                break;
            default:
                break;
        }
    }
}
public void processRequestCommand(final ChannelHandlerContext ctx, final RemotingCommand cmd) {
    final Pair<NettyRequestProcessor, ExecutorService> matched = this.processorTable.get(cmd.getCode());
    final Pair<NettyRequestProcessor, ExecutorService> pair = null == matched ? this.defaultRequestProcessor : matched;
    Runnable run = new Runnable() {
        final RemotingCommand response = pair.getObject1().processRequest(ctx, cmd);
    }
    final RequestTask requestTask = new RequestTask(run, ctx.channel(), cmd);
    pair.getObject2().submit(requestTask);
}
public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException {
    // 解析请求参数
    SendMessageRequestHeader requestHeader = parseRequestHeader(request);
    response = this.sendMessage(ctx, request, mqtraceContext, requestHeader);
}
private RemotingCommand sendMessage(final ChannelHandlerContext ctx,
                                    final RemotingCommand request,
                                    final SendMessageContext sendMessageContext,
                                    final SendMessageRequestHeader requestHeader){
    // 设置请求处理返回对象标志
    final RemotingCommand response = RemotingCommand.createResponseCommand(SendMessageResponseHeader.class);
    response.setOpaque(request.getOpaque());
     // 调用存储模块存储消息
    putMessageResult = this.brokerController.getMessageStore().putMessage(msgInner);
}
public PutMessageResult DefaultMessageStore#putMessage(MessageExtBrokerInner msg) {
    // 如果当前Broker停止工作则拒绝消息写入
    // Broker为SLAVE角色则拒绝消息写入
    // 当前Rocket不支持写入则拒绝消息写入
    //主题长度超过256个字符则拒绝消息写入
	//消息属性长度超过65536个字符则拒绝消息写入
    //调用commitLog存入消息
    PutMessageResult result = this.commitLog.putMessage(msg);
}
public PutMessageResult CommitLog#putMessage(final MessageExtBrokerInner msg) {
    // 果消息的延迟级别大于0，将消息的原主题名称与原消息队列ID存入消息属性中，
    // 用延迟消息主题SCHEDULE_TOPIC、消息队列ID更新原先消息的主题与队列，这是并发消息消费重试关键的一步
    // 获取当前可以写入的Commitlog文件
    MappedFile unlockMappedFile = null;
    MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile();
    //在写入CommitLog之前，先申请putMessageLock，也就是将消息存储到CommitLog文件中是串行的。
    putMessageLock.lock();
    //设置消息的存储时间，如果mappedFile为空，表明${ROCKET_HOME}/store/commitlog目录下不存在任何文件，说明本次消息是第一次消息发送，用偏移量0创建第一个commit文件
    if (null == mappedFile || mappedFile.isFull()) {
        mappedFile = this.mappedFileQueue.getLastMappedFile(0); 
    }
    result = mappedFile.appendMessage(msg, this.appendMessageCallback);
    // KEYPOINT 文件刷盘
    handleDiskFlush(result, putMessageResult, msg);
    // 高可用服务
    handleHA(result, putMessageResult, msg);
}
public AppendMessageResult appendMessage(final MessageExtBrokerInner msg, final AppendMessageCallback cb) {
    return appendMessagesInner(msg, cb);
}
//将消息追加到MappedFile中
public AppendMessageResult appendMessagesInner(final MessageExt messageExt, final AppendMessageCallback cb) {
    assert messageExt != null;
    assert cb != null;

    // 首先先获取MappedFile当前写指针，
    int currentPos = this.wrotePosition.get();  //当前文件的写指针
    if (currentPos < this.fileSize) {
        // 如果currentPos小于文件大小，通过slice（）方法创建一个与MappedFile的共享内存区，并设置position为当前指针。
        // writeBuffer 表示从 DM 中申请的缓存；mappedByteBuffer 表示从 PageCache中申请的缓存
        // 当设置读写分离时，初始化会初始化 writeBuffer，此时 writeBuffer 不为空则会从writeBuffer也就是直接内存中申请缓存；
        ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice();
        byteBuffer.position(currentPos);
        AppendMessageResult result = null;
        // 根据是否时批量消息分别处理
        if (messageExt instanceof MessageExtBrokerInner) {
            result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBrokerInner) messageExt);
        } else if (messageExt instanceof MessageExtBatch) {
            result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBatch) messageExt);
        } else {
            return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR);
        }
        this.wrotePosition.addAndGet(result.getWroteBytes());
        this.storeTimestamp = result.getStoreTimestamp();
        return result;
    }
    //如果currentPos大于或等于文件大小则表明文件已写满，抛出AppendMessageStatus.UNKNOWN_ERROR。
    log.error("MappedFile.appendMessage return null, wrotePosition: {} fileSize: {}", currentPos, this.fileSize);
    return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR);
}
public AppendMessageResult doAppend(final long fileFromOffset, final ByteBuffer byteBuffer, final int maxBlank,final MessageExtBrokerInner msgInner) {
    // PHY OFFSET 查找即将写入的消息物理Offset。
    // 获取该消息在消息队列的偏移量。CommitLog中保存了当前所有消息队列的当前待写入偏移量。
    // 事务消息单独处理。这里主要处理Prepared类型和Rollback类型的消息，设置消息queueOffset为0。
    // 如果消息长度+END_FILE_MIN_BLANK_LENGTH大于CommitLog文件的空闲空间，
    // 则返回AppendMessageStatus.END_OF_FILE, Broker会重新创建一个新的CommitLog文件来存储该消息。
    // 设置消息的各个字段内容
    //将消息内容存储到ByteBuffer中，然后创建AppendMessageResult。
    // 这里只是将消息存储在MappedFile对应的内存映射Buffer中，并没有刷写到磁盘
    final long beginTimeMills = CommitLog.this.defaultMessageStore.now();
    // Write messages to the queue buffer
    // 这里只是将消息存储在MappedFile对应的内存映射Buffer中，并没有刷写到磁盘
    byteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen);
}
```

### 4、内存映射

**1.Rocket 内存映射概述**

CommitLog：RocketMQ 对存储消息的物理文件的抽象实现，也就是物理CommitLog文件的具体实现。

MappedFile：CommitLog文件在内存中的映射文件，映射文件同时具有内存的写入速度和与磁盘一样可靠的持久化方式。MappedFile 持有 fileChannel、writeBuffer、mappedByteBuffer。

MappedFileQueue：映射文件队列中有全部的CommitLog映射文件，第一个映射文件为最先过期的文件，最后一个文件是最后过期的文件，最新的消息总是写入最后一个映射文件。

MappedFile 持有 fileChannel、mappedByteBuffer、writeBuffer。fileChannel 文件通道对应 sendFile，mappedByteBuffer 对应 mmap，两者都都可以理解为写入 PageCache。writeBuffer 在 transientStorePoolEnable为true启用，是通过ByteBuffer 分配直接内存，并锁定在内存中（不换到虚拟内存）

![image-20210721194712973](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210721194712973.png)

**2.PageCache**

顺序读写 PageCache 异步刷盘：页缓存（PageCache) 是 OS 对文件的缓存，用于加速对文件的读写。一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于 OS 使用 PageCache 机制对读写访问操作进行了性能优化，将一部分的内存用作 PageCache。对于数据的写入，OS 会先写入至 PageCache 内，随后通过异步的方式由 pdflush 内核线程将 pageCache 内的数据刷盘至物理磁盘上。

预读取：对于数据的读取，如果一次读取文件时出现未命中 PageCache 的情况，OS 从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取。

在 RocketMQ 中，ConsumeQueue 逻辑消费队列存储的数据较少，并且是顺序读取，在 PageCache 机制的预读取作用下，ConsumeQueue 文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。而对于 CommitLog 消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取，严重影响性能。如果选择合适的系统 I/O 调度算法，比如设置调度算法为“Deadline”（此时块存储采用 SSD 的话），随机读的性能也会有所提升。

**2.零拷贝**

RocketMQ 主要通过 MappedByteBuffer 对文件进行读写操作。其中，利用了 NIO 中的 FileChannel 将磁盘上的物理文件直接映射到用户态的内存地址中（这种 mmap 的方式减少了传统 I/O 将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（正因为需要使用内存映射机制，故 RocketMQ 的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存）。

Linux 操作系统分为“用户态”和“内核态”，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制，一台服务器把本机磁盘文件的内容发送到客户端，一般分为两个步骤：
- read(file, tmp_buf, len);，读取本地文件内容；
- write(socket, tmp_buf, len);，将读取的内容通过网络发送出去。

tmp_buf 是预先申请的内存，这两个看似简单的操作，实际进行了 4 次数据复制，分别是：从磁盘复制数据到内核态内存，从内核态内存复制到用户态内存（完成了 read(file, tmp_buf, len)）；然后从用户态内存复制到网络驱动的内核态内存，最后是从网络驱动的内核态内存复制到网卡中进行传输（完成 write(socket,tmp_buf, len)）。

通过使用 Mmap 的方式，可以省去向用户态的内存复制，提高速度。这种机制在 Java 中是通过 MappedByteBuffer 实现的，RocketMQ 通过 mmap 方式优化文件读写性能。

零拷贝详细内容参考：《操作系统I/O设备管理》

**4.源码解析**

```java
// CommitLog 持有 MappedFileQueue
private final MappedFileQueue mappedFileQueue;
// MappedFileQueue 持有 MappedFile文件集合。
private final CopyOnWriteArrayList<MappedFile> mappedFiles = new CopyOnWriteArrayList<MappedFile>();
// 在CommitLog 中通过 mappedFileQueue 获取当前可以写入的MappedFile文件
MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile();
// CommitLog 中调用 mappedFile.appendMessage() 写入消息内容；
result = mappedFile.appendMessage(msg, this.appendMessageCallback);
// 获取对应的 byteBuffer(DM or mmap)
ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice();
// 这里只是将消息存储在 MappedFile 对应的内存映射Buffer中，并没有刷写到磁盘
byteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen);
// PageCache 刷盘，
CommitLog.this.mappedFileQueue.flush(0);
CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages);
this.fileChannel.force(false);
this.mappedByteBuffer.force();
```




### 5、消息刷盘

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_2.png)

- 同步刷盘：如上图所示，只有在消息真正持久化至磁盘后 RocketMQ 的 Broker 端才会真正返回给 Producer 端一个成功的 ACK 响应。同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是性能上会有较大影响，一般适用于金融业务应用该模式较多。
- 异步刷盘：能够充分利用 OS 的 PageCache 的优势，只要消息写入 PageCache 即可将成功的 ACK 返回给 Producer 端。消息刷盘采用后台异步线程提交的方式进行，降低了读写延迟，提高了 MQ 的性能和吞吐量。

刷盘实现

![image-20210721191106188](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210721191106188.png)

GroupCommitService——同步刷盘服务。在Broker存储消息到Page Cache后，同步将Page Cache刷到磁盘，再返回客户端消息并写入结果。

FlushRealTimeService——异步刷盘服务。在Broker存储消息到Page Cache后，立即返回客户端写入结果，然后异步刷盘服务将Page Cache异步刷到磁盘。

CommitRealTimeService——异步转存服务。Broker通过配置读写分离将消息写入直接内存（Direct Memory，简称 DM），然后通过异步转存服务，将DM 中的数据再次存储到 Page Cache中，以供异步刷盘服务将Page Cache刷到磁盘中，转存服务过程如图6-13所示。

```java
public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) {
    //  
    final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;
    // 构建 GroupCommitRequest 同步任务并提交到 GroupCommitRequest
    GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes());
    //保存同步刷盘请求
    service.putRequest(request); 
    
    //如果transientStorePoolEnable为true, RocketMQ会单独申请一个与目标物理文件（commitlog）同样大小的堆外内存，
    // 该堆外内存将使用内存锁定，确保不会被置换到虚拟内存中去，消息首先追加到堆外内存，然后提交到与物理文件的内存映射内存中，再flush到磁盘。
    // 如果transientStorePoolEnable为flalse，消息直接追加到与物理文件直接映射的内存中，然后刷写到磁盘中
    if (!this.defaultMessageStore.getMessageStoreConfig().isTransientStorePoolEnable()) {
        flushCommitLogService.wakeup();
    } else {
        commitLogService.wakeup();
    }
}	
class GroupCommitService extends FlushCommitLogService {
    private void doCommit() {
        //遍历同步刷盘任务列表，根据加入顺序逐一执行刷盘逻辑。
        //判断当前请求刷盘的请求是否已经刷盘由于操作系统刷盘耗时及每次刷多少字节数据到磁盘等，都不是RocketMQ进程能掌控的，所以在每次刷盘前都需要做必要的检查，以确认当前同步刷盘请求对应位点的消息是否已经被刷盘，如果已经被刷盘，当前刷盘请求就不需要执行。
        //执行刷盘：调用mappedFileQueeu#flush方法执行刷盘操作，最终会调用MappedByte Buffer# force（）方法
        CommitLog.this.mappedFileQueue.flush(0);
        //处理完所有同步刷盘任务后，更新刷盘检测点StoreCheckpoint中的physicMsg-Timestamp，
        // 但并没有执行检测点的刷盘操作，刷盘检测点的刷盘操作将在刷写消息队列文件时触发。
    }
}
class FlushRealTimeService extends FlushCommitLogService {
    public void run() {
        //是否定时刷盘，设置为True表示定时刷盘；设置为False表示实时刷盘。默认为False，即实时刷盘。
        //在Broker中配置项名是flushIntervalCommitLog，刷盘间隔默认为500ms。
        //如果距上次提交间隔超过flushPhysicQueueThoroughInterval，则本次刷盘任务将忽略flushPhysicQueueLeastPages，也就是如果待刷写数据小于指定页数也执行刷写磁盘操作
        //执行刷盘：调用flush方法将内存中数据刷写到磁盘，并且更新存储检测点文件的commitlog文件的更新时间戳，文件检测点文件（checkpoint文件）的刷盘动作在刷盘消息消费队列线程中执行
        CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages);
    }
}
class CommitRealTimeService extends FlushCommitLogService {
    public void run() {
        //获取转存参数。整个转存过程的参数都是可配置的
        //对应的配置项名字是 commitIntervalCommitLog，转存操作线程两次执行操作的时间间隔默认为200ms。最小转存Page Cache的Page数，默认为4。
        // 执行转存
        boolean result = CommitLog.this.mappedFileQueue.commit(commitDataLeastPages);
        if (!result) {
            //now wake up flush thread.唤醒异步刷盘线程
            flushCommitLogService.wakeup();
        }
        //转存失败，唤醒异步刷盘线程。转存数据失败，并不代表没有数据被转存到Page Cache中，而是说明有部分数据转存成功，部分数据转存失败。所以可以唤醒刷盘线程执行刷盘操作。而如果转存成功，则正常进行异步刷盘即可
    }
}
```

### 6、索引构建

Broker中有两种索引：Consumer Queue和Index File。，ConsumerQueue 主要用于消费拉取消息、更新消费位点等所用的索引。Index File 是一个 RocketMQ 实现的 Hash 索引，主要在用户用消息key查询时使用。

**1.索引的构建过程**

ConsumeQueue 和 IndexFile两个索引都是由 ReputMessageService 线程构建，ReputMessageService 继承 ServiceThread，ReputMessageService 线程每执行一次任务推送休息 1 毫秒就继续尝试调用 doReput() 推送消息到消息消费队列和索引文件。

第一步：从CommitLog中查找未创建索引的消息，将消息组装成DispatchRequest对象，该逻辑主要在CommitLog#checkMessageAndReturnSize() 方法中实现。

第二步：调用 doDispatch() 方法，该方法会循环多个索引处理器（这里初始化了CommitLogDispatcherBuildConsumeQueue 和 CommitLogDispatcherBuildIndex 两个索引处理器）并调用索引处理器的 dispatch() 方法来处理 DispatchRequest。
CommitLogDispatcherBuildConsumeQueue 索引处理器用于构建 ConsumeQueue，CommitLogDispatcherBuildIndex 用于构建 Index file。ConsumeQueue 是必须创建的，IndexFile 是否需要创建则是通过设置 messageIndexEnable 为 True 或 False 来实现的，默认为 True。

```java
private void doReput() {
    //从CommitLog中查找未创建索引的消息，将消息组装成DispatchRequest对象
    DispatchRequest dispatchRequest = DefaultMessageStore.this.commitLog.checkMessageAndReturnSize(result.getByteBuffer(), false, false);
    DefaultMessageStore.this.doDispatch(dispatchRequest);
}
// 调用CommitLogDispatcherBuildConsumeQueue 和 CommitLogDispatcherBuildIndex 两个索引处理器的dispatch（）方法来处理DispatchRequest。                                
public void doDispatch(DispatchRequest req) {
    for (CommitLogDispatcher dispatcher : this.dispatcherList) {
        dispatcher.dispatch(req);
    }
}
public void putMessagePositionInfo(DispatchRequest dispatchRequest) {
    //根据消息主题与队列ID，先获取对应的ConumeQueue文件
    //每一个消息主题对应一个消息消费队列目录，然后主题下每一个消息队列对应一个文件夹，取出该文件夹最后的ConsumeQueue文件即可
    ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId());
    cq.putMessagePositionInfoWrapper(dispatchRequest);
}
public void putMessagePositionInfoWrapper(DispatchRequest request) {
    boolean result = this.putMessagePositionInfo(request.getCommitLogOffset(),
             request.getMsgSize(), tagsCode, request.getConsumeQueueOffset());
}
// ConsumeQueue 也是通过 MappedFileQueue/MappedFile 存储，刷盘方式固定为异步刷盘模式。
private boolean putMessagePositionInfo(final long offset, final int size, final long tagsCode,final long cqOffset) {
    // 依次将消息偏移量、消息长度、tag hashcode写入到ByteBuffer中，并根据consumeQueueOffset计算ConumeQueue中的物理地址，
    // 将内容追加到ConsumeQueue的内存映射文件中（本操作只追击并不刷盘）, ConumeQueue的刷盘方式固定为异步刷盘模式。
    return mappedFile.appendMessage(this.byteBufferIndex.array());
}
```

**2.按照位点查消息**

Broker 在处理客户端拉取消息请求时是怎么查询消息的

![image-20210722154110395](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210722154110395.png)

getMessage（）方法查询消息的过程可以分为以下几个步骤。

第一步：拉取前校验。校验 DefaultMessageStore 服务是否已经关闭（正常关闭进程时会被关闭），校验DefaultMessageStore 服务是否可读。
第二步：根据 Topic 和 queueId 查找 ConsumeQueue 索引映射文件。判断根据查找到的 ConsumeQueue 索引文件校验传入的待查询的位点值是否合理，如果不合理，重新计算下一次可以拉取的位点值。
第三步：循环查询满足 maxMsgNums 条数的消息。循环从 ConsumeQueue 中读取消息物理位点、消息大小和消息 Tag 的 Hash 值。先做 Hash 过滤，再使用过滤后的消息物理位点到 CommitLog 中查找消息体，并放入结果列表中。
第四步：监控指标统计，返回拉取的消息结果。

```java
 public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset,final int maxMsgNums,final MessageFilter messageFilter) {
     //拉取前校验。校验DefaultMessageStore服务是否已经关闭（正常关闭进程时会被关闭），校验DefaultMessageStore服务是否可读。
     //根据 Topic 和 queueId 查找 ConsumeQueue 索引映射文件
     ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId);
 }
```

**3.按照时间段查消息**

这是社区提供的管理平台的功能，输入 Topic、起始时间、结束时间可以查到这段时间内的消息。这是一个根据ConsumeQueue索引查询消息的扩展查询，具体步骤如下：
第一步：查找这个 Topic 的所有 Queue。
第二步：在每一个队列中查找起始时间、结束时间对应的起始 offset 和最后消息的offset。如何根据时间查找物理位点呢？主要在于构建ConsumeQueue，这个文件是按照时间顺序写的，每条消息的索引数据结构大小是固定 20字节。可以根据时间做二分折半搜索，找到与时间最接近的一个位点。具体实现逻辑在ConsumeQueue.getOffsetInQueueByTime() 方法中。
第三步：根据起始位点、最后消息位点和 Topic，循环拉取所有 Queue就可以拉取到消息。

**4.按照key查询消息**

如果通过设置 messageIndexEnable=True（默认是True）来开启 Index 索引服务，那么在写入消息时会根据 key自动构建 Index File索引。用户可以通过 Topic 和 key 查询消息，查询方法为ConsumeQueue.queryMessage()。queryMessage() 方法的查询过程与按照位点查询消息的过程类似，下面简单介绍该方法的实现过程：
第一步：调用 indexService.queryOffset（）方法，通过 Topic、key 查找目标消息的物理位点信息。
第二步：根据物理位点信息在CommitLog中循环查找消息体内容。
第三步：返回查询结果。

### 7、文件删除

RocketMQ中主要保存了CommitLog、Consume Queue、Index File三种数据文件。由于内存和磁盘都是有限的资源，Broker不可能永久地保存所有数据，RocketMQ 通过设置数据过期时间来删除额外的数据文件，一些超过保存期限的数据会被定期删除。

**1.CommitLog 删除**

CommitLog文件由 DefaultMessageStore.CleanCommitLogService 类提供的一个线程服务周期执行删除操作，实现代码如下：

```java
public void run() {
    //删除过期文件，当满足三个条件时执行删除操作。
    //第一，当前时间等于已经配置的删除时间。第二，磁盘使用空间超过85%。第三，手动执行删除
    this.deleteExpiredFiles();
    this.redeleteHangedFile();
}
```

deleteExpiredFile() 方法直接调用了this.mappedFileQueue.deleteExpiredFileByTime() 方法

第一步：克隆全部的 CommitLog 文件。CommitLog 文件可能随时有数据写入，为了不影响正常写入，所以克隆一份来操作。
第二步：检查每一个 CommitLog 文件是否过期，如果已过期则立即通过调用destroy()方法进行删除。在删除前会做一系列检查：检查文件被引用的次数、清理映射的所有内存数据对象、释放内存。清理完成后，删除物理文件。

**2.索引文件删除**



## 消息查询

### 1、消费者启动


![pullcomsumer.start](https://gitee.com/bruceyum/pictures/raw/master/pics/ab4887ee875da4fd072f9cd551dd12c9)

第一步：最初创建defaultMQPullConsumerImpl时的状态为ServiceState.CREATE_JUST，然后设置消费者的默认启动状态为失败。

第二步：检查消费者的配置比，如消费者组名、消费类型、Queue分配策略等参数是否符合规范；将订阅关系数据发给Rebalance服务对象。

第三步：校验消费者实例名，如果是默认的名字，则更改为当前的程序进程id。

第四步：获取一个 MQClientInstance，如果 MQClientInstance 已经初始化，则直接返回已初始化的实例。这是核心对象，每个clientId缓存一个实例。

第五步：设置Rebalance对象消费者组、消费类型、Queue分配策略、MQClientInstance等参数。

第六步：对 Broker API 的封装类 pullAPIWrapper进行初始化，同时注册消息，过滤filter。

第七步：初始化位点管理器，并加载位点信息。位点管理器分为本地管理和远程管理两种，集群消费时消费位点保存在 Broker 中，由远程管理器管理；广播消费时位点存储在本地，由本地管理器管理。

第八步：本地注册消费者实例，如果注册成功，则表示消费者启动成功。

第九步：启动MQClientInstance实例，启动过程核生产者启动一致。

DefaultMQPushConsumer 的启动过程分为11个步骤，前7个步骤与DefaultMQPullConsumer的步骤类似，不再赘述。

第八步：初始化消费服务并启动。之所以用户“感觉”消息是 Broker 主动推送给自己的，是因为DefaultMQPushConsumer通过Pull服务将消息拉取到本地，再通过Callback的 形 式，将本地消息Push给用户的消费代码。DefaultMQPushConsumer 与DefaultMQPullConsumer获取消息的方式一样，本质上都是拉取。

第十步：更新本地订阅关系和路由信息；通过 Broker 检查是否支持消费者的过滤类型；向集群中的所有Broker发送消费者组的心跳信息。

第十一步：立即执行一次Rebalance，Rebalance过程我们在下一节中详细讲解。

### 2、消息拉取

![image-20210724152842047](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210724152842047.png)

1. 初始化 Push消费者实例。业务代码初始化 DefaultMQPushConsumer实例，启动Pull服务PullMessageService。该服务是一个线程服务，不断执行run（）方法拉取已经订阅Topic的全部队列的消息。
2. 进行本地流控、订阅关系检查，获取Broker信息，调用 pullMessageAsync() 将信息发送到服务器。pullMessageAsync 内部封装一个 RemotintCommand 发送请求，通过NettyChannel将请求发送到NettyServer。
3. Broker收到获取请求的消息，调用store的getMessage()获取消息并返回给consumer，consumer 端 NettyClient收到响应后调用pullCallback.onSuccess()方法。
4. pullCallback.onSuccess()  将pullRequest再次放入pullRequestQueue中（pullService从中获取请求执行拉取任务）。然后将消息保存在本地的缓存队列 ProcessQueue 中，然后提交任务到consumeMessageService。
5. 消费消息。由消费服务 ConsumeMessageConcurrentlyService 或者ConsumeMessageOrderlyService  通过listener机制（业务调用方实现的listener）将本地缓存队列中的消息不断放入消费线程池，异步回调业务消费代码，此时业务代码可以消费消息。ConsumeMessageConcurrentlyService 或者ConsumeMessageOrderlyService 区别在于 ConsumeMessageOrderlyService  会将本地缓存的消息按照MessageId排序后返回给消费者线程。

6. 消费结果处理。业务代码消费后，将消费结果返回给消费服务，再由消费服务将消费进度保存在本地，由消费进度管理服务定时和不定时地持久化到本地（LocalFileOffsetStore支持）或者远程Broker（RemoteBrokerOffsetStore支持）中。
7. 对于消费失败的消息，RocketMQ客户端处理后发回给Broker，并告知消费失败，进入重试消息队列。

```java
// 第一步：初始化 Push消费者实例。业务代码初始化DefaultMQPushConsumer实例，启动Pull服务PullMessageService。该服务是一个线程服务，不断执行run（）方法拉取已经订阅Topic的全部队列的消息，将消息保存在本地的缓存队列中。
public void run() {
    while (!this.isStopped()) {

        // 从pullRequestQueue中获取一个PullRequest消息拉取任务，如果pullRequest Queue为空，则线程将阻塞，直到有拉取任务被放入。
        // 每个Consumer 会负责部分 pullRequest, pullRequest对应消费者组，topic等，通过重负载均服务将pullRequest添加到 pullRequestQueue
        PullRequest pullRequest = this.pullRequestQueue.take();
        this.pullMessage(pullRequest);
    }
}
private void pullMessage(final PullRequest pullRequest) {
    final MQConsumerInner consumer = this.mQClientFactory.selectConsumer(pullRequest.getConsumerGroup());
    DefaultMQPushConsumerImpl impl = (DefaultMQPushConsumerImpl) consumer;
    impl.pullMessage(pullRequest);
}
public void pullMessage(final PullRequest pullRequest) {
    final ProcessQueue processQueue = pullRequest.getProcessQueue();
    //校验ProcessQueue是否dropped；校验消费者服务状态是否正常；校验消费者是否被挂起。
    if (processQueue.isDropped()) { return;}
    // 拉取条数、字节数限制检查。
    // 如果本地缓存消息数量大于配置的最大拉取条数（默认为1000，可以调整），则延迟一段时间再拉取；
    // 如果本地缓存消息字节数大于配置的最大缓存字节数，则延迟一段时间再拉取。两种校验方式都相当于本地流控
    // 订阅关系校验。如果待拉取的Topic在本地缓存中订阅关系为空，则本次拉取不执行，待下一个正常心跳或者Rebalance后订阅关系恢复正常，方可正常拉取。
    // 封装拉取请求和拉取后的回调对象 PullCallback。这里主要将消息拉取请求和拉取结果处理封装成 PullCallback，
    // 并通过调用 PullAPIWrapper.pullKernelImpl（）方法将拉取请求发出去
    this.pullAPIWrapper.pullKernelImpl(/*参数略*/);
}
public PullResult pullKernelImpl(
        final MessageQueue mq,          //从哪个消息消费队列拉取消息
        final String subExpression,     //消息过滤表达式。
        final String expressionType,    //消息表达式类型，分为TAG、SQL92。
        final long subVersion,          //
        final long offset,              //消息拉取偏移量。
        final int maxNums,              //本次拉取最大消息条数，默认32条。
        final int sysFlag,              //拉取系统标记。
        final long commitOffset,        //当前MessageQueue的消费进度（内存中）。
        final long brokerSuspendMaxTimeMillis,//当前MessageQueue的消费进度（内存中）。
        final long timeoutMillis,       //消息拉取超时时间。
        final CommunicationMode communicationMode,//消息拉取模式，默认为异步拉取。
        final PullCallback pullCallback //从Broker拉取到消息后的回调方法。
    ) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
     // 据brokerName、BrokerId从MQClientInstance中获取Broker地址，在整个RocketMQ Broker的部署结构中，相同名称的Broker构成主从结构，其BrokerId会不一样，在每次拉取消息后，会给出一个建议，下次拉取从主节点还是从节点拉取。
    FindBrokerResult findBrokerResult = this.mQClientFactory.findBrokerAddressInSubscribe(mq.getBrokerName(),this.recalculatePullFromWhichNode(mq), false);
    // 拉取请求封装
    PullResult pullResult = this.mQClientFactory.getMQClientAPIImpl().pullMessage(
        brokerAddr,
        requestHeader,
        timeoutMillis,
        communicationMode,
        pullCallback);

    return pullResult;
}
public PullResult pullMessage(/*参数略*/){
	RemotingCommand request = RemotingCommand.createRequestCommand(RequestCode.PULL_MESSAGE, requestHeader);
    case ASYNC:
        this.pullMessageAsync(addr, request, timeoutMillis, pullCallback);
        return null;
}
private void pullMessageAsync(final String addr,final RemotingCommand request,final long timeoutMillis,final PullCallback pullCallback) throws RemotingException, InterruptedException {
    this.remotingClient.invokeAsync(addr, request, timeoutMillis, new InvokeCallback() {
        //MQClientAPIImpl#pullMessageAsync,NettyRemoting-Client在收到服务端响应结构后会回调PullCallback的onSuccess或onException,
        @Override
        public void operationComplete(ResponseFuture responseFuture) {
            RemotingCommand response = responseFuture.getResponseCommand();
            if (response != null) {
                try {
                    PullResult pullResult = MQClientAPIImpl.
                        this.processPullResponse(response);
                    assert pullResult != null;
                    pullCallback.onSuccess(pullResult);
                } catch (Exception e) {
                    pullCallback.onException(e);
                }
            }
        }});
}                        
public void invokeAsync(String addr, RemotingCommand request, long timeoutMillis, InvokeCallback invokeCallback){
    this.invokeAsyncImpl(channel, request, timeoutMillis - costTime, invokeCallback);
}
public void invokeAsyncImpl(final Channel channel, final RemotingCommand request, final long timeoutMillis,final InvokeCallback invokeCallback){
    final ResponseFuture responseFuture = new ResponseFuture(channel, opaque, timeoutMillis - costTime, invokeCallback, once);
    this.responseTable.put(opaque, responseFuture);
    try {
        channel.writeAndFlush(request).addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture f) throws Exception {
                if (f.isSuccess()) {
                    responseFuture.setSendRequestOK(true);
                    return;
                }
                requestFail(opaque);
            }
        });
    } 
}
public void processMessageReceived(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception {
    case RESPONSE_COMMAND:
    	processResponseCommand(ctx, cmd);
}
public void processResponseCommand(ChannelHandlerContext ctx, RemotingCommand cmd) {
    final int opaque = cmd.getOpaque();
    final ResponseFuture responseFuture = responseTable.get(opaque);
    executeInvokeCallback(responseFuture);
}
private void executeInvokeCallback(final ResponseFuture responseFuture) {
    executor.submit(new Runnable() {
        @Override
        public void run() {
            responseFuture.executeInvokeCallback();
        }
    });
}
// 回调 MQClientAPIImpl#pullMessageAsync().InvokeCallback#operationComplete();             // operationComplete() 中回调 pullCallback.onSuccess(pullResult)         
public void executeInvokeCallback() {
    if (invokeCallback != null) {
        if (this.executeCallbackOnlyOnce.compareAndSet(false, true)) {
            invokeCallback.operationComplete(this);
        }
    }
}
public void DefaultMQPushConsumerImpl#pullMessage(final PullRequest pullRequest) {
    PullCallback pullCallback = new PullCallback() {
        public void onSuccess(PullResult pullResult) {
            // 更新PullRequest的下一次拉取偏移量，如果msgFoundList为空，则立即将PullReqeuest放入到PullMessageService的pullRequestQueue，以便PullMessageSerivce能及时唤醒并再次执行消息拉取。为什么PullStatus.FOUND, msgFoundList还会为空呢？因为在RocketMQ根据TAG消息过滤，在服务端只是验证了TAG的hashcode，在客户端再次对消息进行过滤，故可能会出现msgFoundList为空的情况。
            //首先将拉取到的消息存入ProcessQueue，然后将拉取到的消息提交到Consume-MessageService中供消费者消费，该方法是一个异步方法，也就是PullCallBack将消息提交到ConsumeMessageService中就会立即返回
            boolean dispatchToConsume = processQueue.putMessage(pullResult.getMsgFoundList());
            DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest(
                pullResult.getMsgFoundList(),
                processQueue,
                pullRequest.getMessageQueue(),
                dispatchToConsume);

            //将消息提交给消费者线程之后PullCallBack将立即返回，可以说本次消息拉取顺利完成，
            // 然后根据pullInterval参数，如果pullInterval>0，则等待pullInterval毫秒后将PullRequest对象放入到PullMessageService的pullRequestQueue中，
            // 该消息队列的下次拉取即将被激活，达到持续消息拉取，实现准实时拉取消息的效果。
            if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() > 0) {
                DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest,
                                                                       DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval());
            } else {
                DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);
            }
        }
    }
}
public void submitConsumeRequest(/*参数省略*/){
	this.consumeExecutor.submit(consumeRequest);
}
class ConsumeRequest implements Runnable {
    public void run() {
        //消费前：执行消费前的 hook 和重试消息预处理。消费前的hook可以理解为消费前的消息预处理
        //预处理重试消息队列：拉取的消息来自重试队列将Topic名重置为原来的Topic名，而不用重试Topic名。
        //消费回调：将消息传递给用户编写的业务消费代码进行处理；
        status = listener.consumeMessage(Collections.unmodifiableList(msgs), context);
        // 消息结果处理：包含消费指标统计、消费重试处理和消费位点处理。
        // 消费指标主要是对消费成功和失败的TPS的统计；
        // 消费重试处理主要将消费重试次数加1；
        // 消费位点处理主要根据消费结果更新消费位点记录。
        if (!processQueue.isDropped()) {
            ConsumeMessageConcurrentlyService.this.
                processConsumeResult(status, context, this);
        }
    }
}
```



RocketMQ 支持按照下面两种维度（“按 照Message Id 查询消息”、“按照 Message Key 查询消息”）进行消息查询。

消息长轮询



**按照 MessageId 查询消息**

RocketMQ 中的 MessageId 的长度总共有 16 字节，其中包含了消息存储主机地址（IP地址和端口），消息 CommitLog offset。“按照 MessageId 查询消息”在 RocketMQ 中具体做法是：Client 端从 MessageId 中解析出 Broker 的地址（IP 地址和端口）和 CommitLog 的偏移地址后封装成一个 RPC 请求后通过 Remoting 通信层发送（业务请求码：VIEW_MESSAGE_BY_ID）。Broker 端走的是 QueryMessageProcessor，读取消息的过程用其中的 commitLog offset 和 size 去 commitLog 中找到真正的记录并解析成一个完整的消息返回。

**按照 Message Key 查询消息**

“按照 Message Key 查询消息”，主要是基于 RocketMQ 的 IndexFile 索引文件来实现的。RocketMQ 的索引文件逻辑结构，类似 JDK 中 HashMap 的实现。索引文件的具体结构如下：

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_13.png)

IndexFile 索引文件为用户提供通过“按照 Message Key 查询消息”的消息索引查询服务，IndexFile 文件的存储位置是：$HOME\store\index\${fileName}，文件名 fileName 是以创建时的时间戳命名的，文件大小是固定的，等于 40+500W\*4+2000W\*20= 420000040个字节大小。如果消息的 properties 中设置了 UNIQ_KEY 这个属性，就用 topic + “#” + UNIQ_KEY的value作为 key 来做写入操作。如果消息设置了 KEYS 属性（多个 KEY 以空格分隔），也会用 topic + “#” + KEY 来做索引。

其中的索引数据包含了 Key Hash/CommitLog Offset/Timestamp/NextIndex offset 这四个字段，一共20 Byte。NextIndex offset 即前面读出来的 slotValue，如果有 hash冲突，就可以用这个字段将所有冲突的索引用链表的方式串起来了。Timestamp 记录的是消息 storeTimestamp 之间的差，并不是一个绝对的时间。整个 IndexFile 的结构如图，40 Byte 的 Header 用于保存一些总的统计信息，4\*500W 的 Slot Table 并不保存真正的索引数据，而是保存每个槽位对应的单向链表的头。20\*2000W 是真正的索引数据，即一个 IndexFile 可以保存 2000W个索引。

“按照 Message Key 查询消息”的方式，RocketMQ 的具体做法是，主要通过 Broker 端的 QueryMessageProcessor 业务处理器来查询，读取消息的过程就是用 topic 和 key 找到 IndexFile 索引文件中的一条记录，根据其中的 commitLog offset 从 CommitLog 文件中读取消息的实体内容。

### 3、消息过滤

RocketMQ 分布式消息队列的消息过滤方式有别于其它 MQ 中间件，是在 Consumer 端订阅消息时再做消息过滤的。RocketMQ 这么做是在于其 Producer 端写入消息和 Consumer 端订阅消息采用分离存储的机制来实现的，Consumer 端订阅消息是需要通过 ConsumeQueue 这个消息消费的逻辑队列拿到一个索引，然后再从 CommitLog 里面读取真正的消息实体内容，所以说到底也是还绕不开其存储结构。其 ConsumeQueue 的存储结构如下，可以看到其中有 8 个字节存储的 Message Tag 的哈希值，基于 Tag 的消息过滤正式基于这个字段值的。

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_7.png)

主要支持如下2种的过滤方式

- Tag 过滤方式：Consumer 端在订阅消息时除了指定 Topic 还可以指定 Tag，如果一个消息有多个 Tag，可以用 || 分隔。其中，Consumer 端会将这个订阅请求构建成一个 SubscriptionData，发送一个 Pull 消息的请求给 Broker 端。Broker 端从 RocketMQ 的文件存储层— Store 读取数据之前，会用这些数据先构建一个 MessageFilter，然后传给 Store。Store 从 ConsumeQueue 读取到一条记录后，会用它记录的消息 tag hash 值去做过滤，由于在服务端只是根据 hashcode 进行判断，无法精确对 tag 原始字符串进行过滤，故在消息消费端拉取到消息后，还需要对消息的原始 tag 字符串进行比对，如果不同，则丢弃该消息，不进行消息消费。

- SQL92 的过滤方式：这种方式的大致做法和上面的 Tag 过滤方式一样，只是在 Store 层的具体过滤过程不太一样，真正的 SQL expression 的构建和执行由 rocketmq-filter 模块负责的。每次过滤都去执行 SQL 表达式会影响效率，所以 RocketMQ 使用了 BloomFilter 避免了每次都去执行。SQL92 的表达式上下文为消息的属性。

### 4、负载均衡

RocketMQ 中的负载均衡都在 Client 端完成，具体来说的话，主要可以分为 Producer 端发送消息时候的负载均衡和 Consumer 端订阅消息的负载均衡。

**1. Producer的负载均衡**

- Producer 端在发送消息的时候，会先根据 Topic 找到指定的 TopicPublishInfo;
- 在获取了 TopicPublishInfo 路由信息后，RocketMQ 的客户端在默认方式下 selectOneMessageQueue() 方法会从 TopicPublishInfo 中的 messageQueueList 中选择一个队列（MessageQueue）进行发送消息。
- 具体的容错策略均在 MQFaultStrategy 这个类中定义。这里有一个 sendLatencyFaultEnable 开关变量，如果开启，在随机递增取模的基础上，再过滤掉 not available 的 Broker 代理。
- 所谓的"latencyFaultTolerance"，是指对之前失败的，按一定的时间做退避。例如，如果上次请求的 latency 超过 550Lms，就退避 3000Lms；超过 1000L，就退避 60000L；
- 如果关闭，采用随机递增取模的方式选择一个队列（MessageQueue）来发送消息，latencyFaultTolerance 机制是实现消息发送高可用的核心关键所在。

**2、Consumer 的负载均衡**

在 RocketMQ 中，Consumer 端的两种消费模式（Push/Pull）都是基于拉模式来获取消息的，而在 Push 模式只是对 pull 模式的一种封装，其本质实现为消息拉取线程在从服务器拉取到一批消息后，然后提交到消息消费线程池后，又“马不停蹄”的继续向服务器再次尝试拉取消息。如果未拉取到消息，则延迟一下又继续拉取。在两种基于拉模式的消费方式（Push/Pull）中，均需要 Consumer 端在知道从 Broker 端的哪一个消息队列—队列中去获取消息。因此，有必要在 Consumer 端来做负载均衡，即 Broker 端中多个 MessageQueue 分配给同一个 ConsumerGroup 中的哪些 Consumer 消费。

**Consumer 端的心跳包发送**

- 在 Consumer 启动后，它就会通过定时任务不断地向 RocketMQ 集群中的所有 Broker 实例发送心跳包（其中包含了，消息消费分组名称、订阅关系集合、消息通信模式和客户端 id 的值等信息）。
- Broker 端在收到 Consumer 的心跳消息后，会将它维护在 ConsumerManager 的本地缓存变量—consumerTable，同时并将封装后的客户端网络通道信息保存在本地缓存变量—channelInfoTable 中，为之后做 Consumer 端的负载均衡提供可以依据的元数据信息。

**Consumer 端实现负载均衡的核心类—RebalanceImpl**

在 Consumer 实例的启动流程中的启动 MQClientInstance 实例部分，会完成负载均衡服务线程—RebalanceService 的启动（每隔 20s 执行一次）。通过查看源码可以发现，RebalanceService 线程的 run() 方法最终调用的是 RebalanceImpl 类的 rebalanceByTopic() 方法，该方法是实现 Consumer 端负载均衡的核心。这里，rebalanceByTopic() 方法会根据消费者通信类型为“广播模式”还是“集群模式”做不同的逻辑处理。这里主要来看下集群模式下的主要处理流程：

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_8.png)

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_9.png)

1. 查找当前 clientId 对应的全部的消费者组，全部执行一次 Rebalance。从 rebalanceImpl 实例的本地缓存变量—topicSubscribeInfoTable 中，获取该 Topic 主题下的消息消费队列集合（mqSet）；
2. 根据 topic 和 consumerGroup 为参数调用 mQClientFactory.findConsumerIdList() 方法向 Broker 端发送获取该消费组下消费者 Id 列表的 RPC 通信请求（Broker 端基于前面 Consumer 端上报的心跳包数据而构建的 consumerTable 做出响应返回，业务请求码：GET_CONSUMER_LIST_BY_GROUP）；
3. 先对 Topic 下的消息消费队列、消费者 Id 排序，然后用消息队列分配策略算法（默认为：消息队列的平均分配算法），计算出待拉取的消息队列。这里的平均分配算法，类似于分页的算法，将所有 MessageQueue 排好序类似于记录，将所有消费端 Consumer 排好序类似页数，并求出每一页需要包含的平均 size 和每个页面记录的范围 range，最后遍历整个 range 而计算出当前 Consumer 端应该分配到的记录（这里即为：MessageQueue）。
4. 然后，调用 updateProcessQueueTableInRebalance() 方法，具体的做法是，先将分配到的消息队列集合（mqSet）与 processQueueTable 做一个过滤比对。如果有MessageQueue不再分配给当前的消费者消费，则设置 ProcessQueue.setDropped（true），表示放弃当前MessageQueue 的 Pull 消息，如果有新增的mq分配给该消费者则创建对应的ProcessQueue，创建对应的 pullRequest 加入到pullRequestQueue中。

消息消费队列在同一消费组不同消费者之间的负载均衡，其核心设计理念是在一个消息消费队列在同一时间只允许被同一消费组内的一个消费者消费，一个消息消费者能同时消费多个消息队列。

源码解析：

```java
public void RebalanceService#run() {
    while (!this.isStopped()) {
        this.waitForRunning(waitInterval);
        this.mqClientFactory.doRebalance();
    }
}
 public void MQClientInstance#doRebalance() {
     //第一步：查找当前 clientId 对应的全部的消费者组，全部执行一次 Rebalance。
     for (Map.Entry<String, MQConsumerInner> entry : this.consumerTable.entrySet()) {
         MQConsumerInner impl = entry.getValue();
         //判断 Rebalance 开关，如果没有被暂停，则执行重平衡
         impl.doRebalance();
     }
 }
public void doRebalance() {
    this.rebalanceImpl.doRebalance(this.isConsumeOrderly());
}
public void doRebalance(final boolean isOrder) {
    // 获取当前消费者全部订阅关系中的 Topic，循环对每个Topic进行Rebalance。
    Map<String, SubscriptionData> subTable = this.getSubscriptionInner();
    for (final Map.Entry<String, SubscriptionData> entry : subTable.entrySet()) {
        final String topic = entry.getKey();
        this.rebalanceByTopic(topic, isOrder);
    }
    // 待全部的Rebalance都执行完后，将不属于当前消费者的队列删除。
    this.truncateMessageQueueNotMyTopic();
}
//根据是集群消费还是广播消费分别执行 MessageQueue 重新分配的逻辑
private void rebalanceByTopic(final String topic, final boolean isOrder) {
    case CLUSTERING: {
        //获取当前Topic的全部MessageQueue（代码中是mqSet）和该Topic的所有消费者的clientId（代码中是cidAll）
        Set<MessageQueue> mqSet = this.topicSubscribeInfoTable.get(topic);
        List<String> cidAll = this.mQClientFactory.findConsumerIdList(topic, consumerGroup);
        // 将全部的MessageQueue（代码中是mqAll）和消费者客户端（cidAll）进行排序。
        Collections.sort(mqAll);
        Collections.sort(cidAll);
        //按照当前设置的队列分配策略执行 Queue 分配
        AllocateMessageQueueStrategy strategy = this.allocateMessageQueueStrategy;
        allocateResult = strategy.allocate(this.consumerGroup,
            this.mQClientFactory.getClientId(),mqAll,cidAll);
        //动态更新ProcessQueue。在队列重新分配后，当前消费者消费的队列可能不会发生变化，也可能发生变化，不管是增加了新的队列需要消费，还是减少了队列，都需要执行updateProcessQueueTableInRebalance（）方法来更新ProcessQueue。如果有MessageQueue不再分配给当前的消费者消费，则设置 ProcessQueue.setDropped（true），表示放弃当前MessageQueue 的 Pull 消息
        boolean changed = this.updateProcessQueueTableInRebalance(topic, allocateResultSet, isOrder);
}
private boolean updateProcessQueueTableInRebalance(final String topic, final Set<MessageQueue> mqSet,final boolean isOrder) {
    // 获取本地缓存当前客户端处理的 MessageQueue
    Iterator<Entry<MessageQueue, ProcessQueue>> it = this.processQueueTable.entrySet().iterator();
    //如果不包含在mqSet中，则移除
    pq.setDropped(true);
    // 如果在mqSet, 不在processQueueTable则是新增mq
    if (!this.processQueueTable.containsKey(mq)) {
        //新增一个ProcessQueue用于处理对应新增的mq
        ProcessQueue pq = new ProcessQueue();
        ProcessQueue pre = this.processQueueTable.putIfAbsent(mq, pq);
    }
    // 将新分配的MessageQueue 对应的pullRequse放入拉取任务队列 pullRequestQueue
    this.dispatchPullRequest(pullRequestList);
}
public void dispatchPullRequest(List<PullRequest> pullRequestList) {
    for (PullRequest pullRequest : pullRequestList) {
        this.defaultMQPushConsumerImpl.executePullRequestImmediately(pullRequest);
    }
}
public void executePullRequestImmediately(final PullRequest pullRequest) {
 this.mQClientFactory.getPullMessageService().executePullRequestImmediately(pullRequest);
}
// 消费者会从pullRequestQueue获取拉取请求，并执行拉取任务
public void executePullRequestImmediately(final PullRequest pullRequest) {
    this.pullRequestQueue.put(pullRequest);
}
```

消息消费及负载流程：

![image-20210724153632989](https://gitee.com/bruceyum/pictures/raw/master/pics/image-20210724153632989.png)



### 5、位点管理






## 通信机制

RocketMQ 消息队列集群主要包括 NameServer、Broker(Master/Slave)、Producer、Consumer 4 个角色，基本通讯流程如下：

- Broker 启动后需要完成一次将自己注册至 NameServer 的操作；随后每隔 30s 时间定时向 NameServer 上报 Topic 路由信息。

- 消息生产者 Producer 作为客户端发送消息时候，需要根据消息的 Topic 从本地缓存的 TopicPublishInfoTable 获取路由信息。如果没有则更新路由信息会从 NameServer 上重新拉取，同时 Producer 会默认每隔 30s 向 NameServer 拉取一次路由信息。

- 消息生产者 Producer 根据 NameServer 中获取的路由信息选择一个队列（MessageQueue）进行消息发送；Broker 作为消息的接收者接收消息并落盘存储。

- 消息消费者 Consumer 根据 NameServer 中获取的路由信息，并再完成客户端的负载均衡后，选择其中的某一个或者某几个消息队列来拉取消息并进行消费。

可以看出在消息生产者, Broker 和 NameServer 之间都会发生通信（这里只说了 MQ 的部分通信），因此如何设计一个良好的网络通信模块在 MQ 中至关重要，它将决定 RocketMQ 集群整体的消息传输能力与最终的性能。

rocketmq-remoting 模块是 RocketMQ 消息队列中负责网络通信的模块，它几乎被其他所有需要网络通信的模块（诸如 rocketmq-client、rocketmq-broker、rocketmq-namesrv）所依赖和引用。为了实现客户端与服务器之间高效的数据请求与接收，RocketMQ 消息队列自定义了通信协议并在 Netty 的基础之上扩展了通信模块。

### 1、Remoting通信类结构

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_3.png)

### 2、协议设计与编解码

在 Client 和 Server 之间完成一次消息发送时，需要对发送的消息进行一个协议约定，因此就有必要自定义 RocketMQ 的消息协议。同时，为了高效地在网络中传输消息和对收到的消息读取，就需要对消息进行编解码。在 RocketMQ 中，RemotingCommand 这个类在消息传输过程中对所有数据内容的封装，不但包含了所有的数据结构，还包含了编码解码操作。

Header字段 | 类型 | Request说明 | Response说明
--- | --- | --- | --- |
code |int | 请求操作码，应答方根据不同的请求码进行不同的业务处理 | 应答响应码。0表示成功，非0则表示各种错误
language | LanguageCode | 请求方实现的语言 | 应答方实现的语言
version | int | 请求方程序的版本 | 应答方程序的版本
opaque | int |相当于requestId，在同一个连接上的不同请求标识码，与响应消息中的相对应 | 应答不做修改直接返回
flag | int | 区分是普通 RPC 还是 onewayRPC 得标志 | 区分是普通 RPC 还是 onewayRPC 得标志
remark | String | 传输自定义文本信息 | 传输自定义文本信息
extFields | HashMap<String, String> | 请求自定义扩展信息 | 响应自定义扩展信息

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_4.png)

可见传输内容主要可以分为以下4部分：

(1) 消息长度：总长度，四个字节存储，占用一个int类型；

(2) 序列化类型&消息头长度：同样占用一个int类型，第一个字节表示序列化类型，后面三个字节表示消息头长度；

(3) 消息头数据：经过序列化后的消息头数据；

(4) 消息主体数据：消息主体的二进制字节数据内容；

### 3、通信方式和流程

在RocketMQ消息队列中支持通信的方式主要有同步(sync)、异步(async)、单向(oneway)
三种。其中“单向”通信模式相对简单，一般用在发送心跳包场景下，无需关注其Response。这里，主要介绍RocketMQ的异步通信流程。

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_5.png)

### 4、Reactor 多线程设计

Netty是一个网络应用框架，或者说是一个Java网络开发库。Netty提供异步事件驱动的方式，使用它可以快速地开发出高性能的网络应用程序。Netty主要分为三部分：一是底层的零拷贝技术和统一通信模型；二是基于JVM实现的传输层；三是常用协议支持。RocketMQ的RPC通信采用Netty组件作为底层通信库，同样也遵循了Reactor多线程模型，同时又在这之上做了一些扩展和优化。


![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_6.png)

上面的框图中可以大致了解 RocketMQ 中 NettyRemotingServer 的 Reactor 多线程模型。
- 一个 Reactor 主线程（eventLoopGroupBoss，即为上面的 1）负责监听 TCP网络连接请求，建立好连接，创建SocketChannel，并注册到selector上。RocketMQ的源码中会自动根据 OS 的类型选择 NIO 和 Epoll，也可以通过参数配置）,然后监听真正的网络数据。
- 拿到网络数据后，再丢给Worker线程池（eventLoopGroupSelector，即为上面的“N”，源码中默认设置为3），
- 在真正执行业务逻辑之前需要进行SSL验证、编解码、空闲检查、网络连接管理，这些工作交给defaultEventExecutorGroup（即为上面的“M1”，源码中默认设置为8）去做。
- 而处理业务操作放在业务线程池中执行，根据 RomotingCommand 的业务请求码 code 去 processorTable 这个本地缓存变量中找到对应的 processor，然后封装成task任务后，提交给对应的业务processor处理线程池来执行（sendMessageExecutor，以发送消息为例，即为上面的 “M2”）。
从入口到业务逻辑的几个步骤中线程池一直再增加，这跟每一步逻辑复杂性相关，越复杂，需要的并发通道越宽。

线程数 | 线程名 | 线程具体说明
 --- | --- | --- 
1 | NettyBoss_%d | Reactor 主线程
N | NettyServerEPOLLSelector_%d_%d | Reactor 线程池
M1 | NettyServerCodecThread_%d | Worker线程池
M2 | RemotingExecutorThread_%d | 业务processor处理线程池


## 高可用

主从复制，读写分离，没有 master 选举及故障检测恢复机制

![主从复制](https://img-blog.csdnimg.cn/20190311112501582.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcWl1bWFuMTgwNjg4,size_16,color_FFFFFF,t_70)

RocketMQ 的主从同步机制如下：

1. 首先启动Master并在指定端口监听；
2. 客户端启动，主动连接Master，建立TCP连接；
3. 客户端以每隔5s的间隔时间向服务端拉取消息，如果是第一次拉取的话，先获取本地commitlog文件中最大的偏移量，以该偏移量向服务端拉取消息；
4. 服务端解析请求，并返回一批数据给客户端；
5. 客户端收到一批消息后，将消息写入本地commitlog文件中，然后向Master汇报拉取进度，并更新下一次待拉取偏移量；
6. 然后重复第3步；

RocketMQ 分布式集群是通过 Master 和 Slave 的配合达到高可用性的，首先说一下 Master 和 Slave 的区别：在Broker的配置文件中，参数brokerId的值为0表明这个Broker是Master，大于0表明这个Broker是Slave，同 brokerRole 参数也会说明这个 Broker 是 Master 还是 Slave。Master 角色的 Broker 支持读和写，Slave 角色的 Broker 仅支持读，也就是 Producer 只能和 Master 角色的 Broker 连接写入消息；Consumer 可以连接 Master 角色的 Broker，也可以连接 Slave 角色的 Broker 来读取消息。

在 Consumer 的配置文件中，并不需要设置是从 Master 读还是从 Slave 读，当 Master 不可用或者繁忙的时候，Consumer 会被自动切换到从 Slave 读。有了自动切换 Consumer 这种机制，当一个 Master 角色的机器出现故障后，Consumer 仍然可以从 Slave 读取消息，不影响 Consumer 程序。这就达到了消费端的高可用性。

如何达到发送端的高可用性呢？在创建Topic的时候，把Topic的多个MessageQueue创建在多个Broker组上（相同Broker名称，不同brokerId的机器组成一个Broker组），这样当一个Broker组的Master不可用后，其他组的Master仍然可用，Producer仍然可以发送消息。RocketMQ目前还不支持把Slave自动转成Master，如果机器资源不足，需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文件，用新的配置文件启动Broker。

**主从同步实现**

Master 角色的机器接收到消息后，要把内容同步到 Slave 机器上，这样一旦 Master 宕机，Slave 机器依然可以提供服务。

**1. 元数据同步**

Slave需要和Master同步的不只是消息本身，一些元数据信息也需要同步，比如TopicConfig信息、ConsumerOffset信息、DelayOffset和SubscriptionGroupConfig信息。Broker在启动的时候，判断自己的角色是否是Slave，是的话就启动定时同步任务

定时调用 BrokerController.this.slaveSynchronize.syncAll()，在syncAll函数里，调用syncTopicConfig()、syncConsumerOffset()、syncDelayOffset()和syncSubscriptionGroupConfig()进行元数据同步。

例如，sysConsumerOffset()的基本逻辑是组装一个RemotingCommand，底层通过Netty将消息发送到Master角色的Broker，然后获取Offset信息。

**2. CommitLog同步**

CommitLog的数据量比元数据要大；其次，对实时性和可靠性要求也不一样。元数据信息是定时同步的，在两次同步的时间差里，如果出现异常可能会造成Master上的元数据内容和Slave上的元数据内容不一致，不过这种情况还可以补救（手动调整Offset，重启Consumer等）。CommitLog在高可靠性场景下如果没有及时同步，一旦Master机器出故障，消息就彻底丢失了。

判断角色是 slave, HAClient 试图通过 Java NIO 函数去连接 Master 角色的 Broker, Master 通过 JAVA NIO 方式监听，建立 TCP 连接，这样效率更高。连接成功以后，通过对比 Master 和 Slave 的 Offset，不断进行同步。 

**同步复制和异步复制**

sync_master 和 async_master是写在Broker配置文件里的配置参数，这个参数影响的是主从同步的方式

- sync_master是同步复制方式，是等 Master 和 Slave均写成功后才反馈给客户端写成功状态；
- async_master是异步复制方式，是只要Master写成功即可反馈给客户端写成功状态。

在CommitLog类的putMessage函数末尾，调用handleHA函数。代码中的关键词是wakeupAll和waitForFlush，在同步方式下，Master每次写消息的时候，都会等待向Slave同步消息的过程，同步完成后再返回

这两种复制方式各有优劣，在异步复制方式下，系统拥有较低的延迟和较高的吞吐量，但是如果Master出了故障，有些数据因为没有被写入Slave，有可能会丢失；在同步复制方式下，如果Master出故障，Slave上有全部的备份数据，容易恢复，但是同步复制会增大数据写入延迟，降低系统吞吐量。

同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、SYNC_MASTER、SLAVE三个值中的一个。

实际应用中要结合业务场景，合理设置刷盘方式和主从复制方式，尤其是SYNC_FLUSH方式，由于频繁地触发磁盘写动作，会明显降低性能。通常情况下，应该把Master和Save配置成ASYNC_FLUSH的刷盘方式，主从之间配置成SYNC_MASTER的复制方式，这样即使有一台机器出故障，仍然能保证数据不丢，是个不错的选择。


## 事务消息

Apache RocketMQ在4.3.0版中已经支持分布式事务消息，这里RocketMQ采用了2PC的思想来实现了提交事务消息，同时增加一个补偿逻辑来处理二阶段超时或者失败的消息，如下图所示。

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_10.png)

### 1、事务消息流程概要

上图说明了事务消息的大致方案，其中分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程。

1.事务消息发送及提交：

(1) 发送消息（half消息）。

(2) 服务端响应消息写入结果。

(3) 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。

(4) 根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见）

2.补偿流程：

(1) 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查”

(2) Producer收到回查消息，检查回查消息对应的本地事务的状态

(3) 根据本地事务状态，重新Commit或者Rollback

其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。

### 2、事务消息设计

**1.事务消息在一阶段对用户不可见**

在RocketMQ事务消息的主要流程中，一阶段的消息如何对用户不可见。其中，事务消息相对普通消息最大的特点就是一阶段发送的消息对用户是不可见的。那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后改变主题为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。

在RocketMQ中，消息在服务端的存储结构如下，每条消息都会有对应的索引信息，Consumer通过ConsumeQueue这个二级索引来读取消息实体内容，其流程如下：

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_11.png)

RocketMQ的具体实现策略是：写入的如果事务消息，对消息的Topic和Queue等属性进行替换，同时将原来的Topic和Queue信息存储到消息的属性中，正因为消息主题被替换，故消息并不会转发到该原主题的消息消费队列，消费者无法感知消息的存在，不会消费。其实改变消息主题是RocketMQ的常用“套路”，回想一下延时消息的实现机制。

**2. Commit 和 Rollback 操作以及 Op 消息的引入**

在完成一阶段写入一条对用户不可见的消息后，二阶段如果是Commit操作，则需要让消息对用户可见；如果是Rollback则需要撤销一阶段的消息。先说Rollback的情况。对于Rollback，本身一阶段的消息对用户是不可见的，其实不需要真正撤销消息（实际上RocketMQ也无法去真正的删除一条消息，因为是顺序写文件的）。但是区别于这条消息没有确定状态（Pending状态，事务悬而未决），需要一个操作来标识这条消息的最终状态。RocketMQ事务消息方案中引入了Op消息的概念，用Op消息标识事务消息已经确定的状态（Commit或者Rollback）。如果一条事务消息没有对应的Op消息，说明这个事务的状态还无法确定（可能是二阶段失败了）。引入Op消息后，事务消息无论是Commit或者Rollback都会记录一个Op操作。Commit相对于Rollback只是在写入Op消息前创建Half消息的索引。

**3.Op消息的存储和对应关系**

RocketMQ将Op消息写入到全局一个特定的Topic中通过源码中的方法—TransactionalMessageUtil.buildOpTopic()；这个Topic是一个内部的Topic（像Half消息的Topic一样），不会被用户消费。Op消息的内容为对应的Half消息的存储的Offset，这样通过Op消息能索引到Half消息进行后续的回查操作。

![img](https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_12.png)

**4. Half 消息的索引构建**

在执行二阶段Commit操作时，需要构建出Half消息的索引。一阶段的Half消息由于是写到一个特殊的Topic，所以二阶段构建索引时需要读取出Half消息，并将Topic和Queue替换成真正的目标的Topic和Queue，之后通过一次普通消息的写入操作来生成一条对用户可见的消息。所以RocketMQ事务消息二阶段其实是利用了一阶段存储的消息的内容，在二阶段时恢复出一条完整的普通消息，然后走一遍消息写入流程。

**5.如何处理二阶段失败的消息？**

如果在RocketMQ事务消息的二阶段过程中失败了，例如在做Commit操作时，出现网络问题导致Commit失败，那么需要通过一定的策略使这条消息最终被Commit。RocketMQ采用了一种补偿机制，称为“回查”。Broker端对未确定状态的消息发起回查，将消息发送到对应的Producer端（同一个Group的Producer），由Producer根据消息来检查本地事务的状态，进而执行Commit或者Rollback。Broker端通过对比Half消息和Op消息进行事务消息的回查并且推进CheckPoint（记录那些事务消息的状态是确定的）。

值得注意的是，rocketmq并不会无休止的的信息事务状态回查，默认回查15次，如果15次回查还是无法得知事务状态，rocketmq默认回滚该消息。
