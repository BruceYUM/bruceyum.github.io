事务和锁


[toc]





## 事务简介

### 1、事务概念

事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。

![image_1d7bvq3401fpe1eum1d7qkop1f479.png-69.6kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169d7fd5bd4ce2f1)

- 活动的（active）

  事务对应的数据库操作正在执行过程中时，我们就说该事务处在 活动的 状态。

- 部分提交的（partially committed）

  当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并没有刷新到磁盘时，我们就说该事务处在 部分提交的 状态。

- 失败的（failed）

  当事务处在 活动的 或者 部分提交的 状态时，可能遇到了某些错误（数据库自身的错误、操作系统错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在 失败的 状态。

- 中止的（aborted）

  如果事务执行了半截而变为 失败的 状态，比如我们前边唠叨的狗哥向猫爷转账的事务，当狗哥账户的钱被扣除，但是猫爷账户的钱没有增加时遇到了错误，从而当前事务处在了 失败的 状态，那么就需要把已经修改的狗哥账户余额调整为未转账之前的金额，换句话说，就是要撤销失败事务对当前数据库造成的影响。书面一点的话，我们把这个撤销的过程称之为 回滚 。当 回滚 操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了 中止的 状态。

- 提交的（committed）

  当一个处在 部分提交的 状态的事务将修改过的数据都同步到磁盘上之后，我们就可以说该事务处在了 提交的 状态。

### 2、ACID

**1. 原子性（Atomicity）**

事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。

回滚可以用回滚日志（Undo Log）来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。

**2. 一致性（Consistency）**

数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。

**3. 隔离性（Isolation）**

一个事务所做的修改在最终提交以前，对其它事务是不可见的。

**4. 持久性（Durability）**

一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。

系统发生崩溃可以用重做日志（Redo Log）进行恢复，从而实现持久性。与回滚日志记录数据的逻辑修改不同，重做日志记录的是数据页的物理修改。

----

事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系：

- 只有满足一致性，事务的执行结果才是正确的。
- 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。
- 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。
- 事务满足持久化是为了能应对系统崩溃的情况。

<img src="https://gitee.com/bruceyum/pictures/raw/master/pics/image-20191207210437023.png" width="800"/>

### 3、基本语法

**1.声明事务的开始**：BEGIN(或START TRANSACTION);

START TRANSACTION语句后边跟随几个修饰符：

- READ ONLY：标识当前事务是一个只读事务，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。
- READ WRITE：标识当前事务是一个读写事务，也就是属于该事务的数据库操作既可以读取数据，也可以修改数据。

- WITH CONSISTENT SNAPSHOT：启动一致性读

**2.提交整个事务**：COMMIT;

**3.自动提交**：AUTOCOMMIT

自动提交，默认值为ON。MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。

**4.隐式提交**：当我们使用 START TRANSACTION 或者 BEGIN 语句开启了一个事务，或者把系统变量 autocommit 的值设置为 OFF 时，事务就不会进行 自动提交 ，但是如果我们输入了某些语句之后就会触发隐式提交：

- 定义或修改数据库对象的数据定义语言（Data definition language，缩写为： DDL ）：使用 CREATE 、 ALTER 、 DROP 等语句去修改这些所谓的数据库对象时，就会隐式的提交前边语句所属于的事务。
- 隐式使用或修改 mysql 数据库中的表：使用 ALTER USER 、 CREATE USER 、 DROP USER 、 GRANT 、 RENAME USER 、 REVOKE 、 SET PASSWORD 等语句时也会隐式的提交前边语句所属于的事务。
- 事务控制或关于锁定的语句：在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了另一个事务时，会隐式的提交上一个事务
- 加载数据的语句：使用 LOAD DATA 语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。
- 关于 MySQL 复制的一些语句：使用 START SLAVE 、 STOP SLAVE 、 RESET SLAVE 、 CHANGE MASTER TO 等语句时也会隐式的提交前边语句所属的事务。

**5.回滚到事务初始状态**：ROLLBACK;

**6.保存点**

可以在多条事务语句建定义保存点，我们在调用 ROLLBACK 语句时可以指定会滚到哪个点，而不是回滚所有一致性语句。

```sql
SAVEPOINT 保存点名称;
ROLLBACK TO [SAVEPOINT] 保存点名称;
```



## Undo 日志

### 1、undo日志的格式

为了实现事务的 原子性 ， InnoDB 存储引擎在实际进行增、删、改一条记录时，都需要先把对应的 undo日志 记下来。一般每对一条记录做一次改动，就对应着一条 undo日志 ，但在某些更新记录的操作中，也可能会对应着2条 undo日志 。一个事务在执行过程中可能新增、删除、更新若干条记录，也就是说需要记录很多条对应的 undo日志 ，这些 undo日志 会被从 0 开始编号，也就是说根据生成的顺序分别被称为 第0号undo日志 、 第1号undo日志 、...、 第n号undo日志 等，这个编号也被称之为 undo no 。这些 undo日志 是被记录到类型为 FIL_PAGE_UNDO_LOG 。

UndoLog 一般是逻辑日志，主要分为两种：

**insert undo log**

代表事务在 insert 新记录时产生的 undo log，只在事务回滚时需要，并且在事务提交后可以被立即丢弃。

**update undo log**

事务在进行 update 或 delete 时产生的 undo log。不仅在事务回滚时需要，在快照读时也需要。

所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被 purge 线程统一清除。

## Redo 日志

由于磁盘操作很慢，MySQL 访问页面是先写入 Buffer  Pool ，再由后台线程刷入磁盘。但是如果还没有来得及刷入磁盘就 MySQL 就崩溃了，则会丢失更新，事务也就失去了持久性。我们可以事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但是又回到上面的问题，我们可能支持修改了很少的数据却要刷新多个页面，而且磁盘随机 I/O 很慢。为此MySQL 提供了 Redo 日志，Redo 日志只会记录用户修改的数据，例如，第100号页面中偏移量为1000处的那个字节的值 1 改成 2 ，所以 redo 日志占用的空间非常小，并且 Redo 日志是顺序 I/O。

### 1、Redo 日志格式

Redo 日志通用结构

![image_1d36k7d3412oo1c0qcuuben12l79.png-31.3kB](https://gitee.com/bruceyum/pictures/raw/master/pics/1694892fdec61898)

**1.简单的redo日志类型**

对于一些简单的修改， redo 日志中只需要记录一下在某个页面的某个偏移量处修改了几个字节的值，具体被修改的内容是啥就好了，设计 InnoDB 的大叔把这种极其简单的 redo 日志称之为 物理日志 ，并且根据在页面中写入数据的多少划分了几种不同的 redo 日志类型：MLOG_1BYTE、MLOG_2BYTE、MLOG_4BYTE、MLOG_8BYTE、MLOG_WRITE_STRING。例如 MLOG_WRITE_STRING 类型的 redo 日志表示也的offet 偏移量处写入长度为 len 的字符串。

![image_1d3fv8at819jh1m7m1sfb1donvmu16.png-47.2kB](https://gitee.com/bruceyum/pictures/raw/master/pics/1694892fdf3da823)

**2.复杂的redo日志类型**

以一条 INSERT 语句为例，它除了要向 B+ 树的页面中插入数据，表中包含多少个索引，一条 INSERT 语句就可能更新多少棵 B+ 树。针对某一棵 B+ 树来说，既可能更新叶子节点页面，也可能更新内节点页面，也可能创建新的页面（在该记录插入的叶子节点的剩余空间比较少，不足以存放该记录时，会进行页面的分裂，在内节点页面中添加 目录项记录 ）。 INSERT 语句对所有页面的修改都得保存到 redo 日志中去。 InnoDB 的设计了一些新的 redo 日志类型，比如： MLOG_REC_INSERT 、 MLOG_COMP_REC_INSERT 、 MLOG_COMP_PAGE_CREATE 、 MLOG_COMP_REC_DELETE 、 MLOG_COMP_LIST_START_DELETE 、 MLOG_COMP_LIST_END_DELETE 等。以插入一条记录为例：

![image_1d3bn8tsq1ssp1nmdks8kdr17e31t.png-85.7kB](https://gitee.com/bruceyum/pictures/raw/master/pics/1694892fe02553d0)

### 2、Redo 日志组

**1. 以组的形式写入redo日志**

语句在执行过程中可能修改若干个页面，可能更新聚簇索引和二级索引对应 B+ 树中的页面，在执行语句的过程中产生的 redo 日志被设计 InnoDB 的大叔人为的划分成了若干个不可分割的组，比如：向聚簇索引对应 B+ 树的页面中插入一条记录时产生的 redo 日志是不可分割的，向某个二级索引对应 B+ 树的页面中插入一条记录时产生的 redo 日志是不可分割的。

在向聚簇索引插入一条记录的时候，乐观插入只需要向叶子节点的一个页面插入一条记录，但是悲观插入可能需要页分裂，可能需要修改目录项记录等。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条 redo 日志。

![image_1d4fkn8gv1n7enuq23kt1n1uvk3n.png-96.9kB](https://gitee.com/bruceyum/pictures/raw/master/pics/16948930147d598e)

所以 MySQL 规定在执行这些需要保证原子性的操作时必须以 组 的形式来记录的 redo 日志，在进行系统崩溃重启恢复时，针对某个组中的 redo 日志，要么把全部的日志都恢复掉，要么一条也不恢复。

MySQL 在分组的最后一条 redo 日志后边加上一条特殊类型的 redo 日志，该类型名称为 MLOG_MULTI_REC_END ， type 字段对应的十进制数字为 31 ，该类型的 redo 日志结构很简单，只有一个 type 字段。所以某个需要保证原子性的操作产生的一系列 redo 日志必须要以一个类型为 MLOG_MULTI_REC_END 结尾

![image_1d4fol2v71fjalphluu1kuf1d8t4h.png-41.4kB](https://gitee.com/bruceyum/pictures/raw/master/pics/16948930164d64a0)

注： type 字段的第一个比特位为 1 ，代表该需要保证原子性的操作只产生了单一的一条 redo 日志，否则表示该需要保证原子性的操作产生了一系列的 redo 日志。

**2.Mini-Transaction的概念**

MySQL 把对底层页面中的一次原子访问的过程称之为一个 Mini-Transaction ，简称 mtr ，比如上边所说的修改一次 Max Row ID 的值算是一个 Mini-Transaction ，向某个索引对应的 B+ 树中插入一条记录的过程也算是一个 Mini-Transaction 。通过上边的叙述我们也知道，一个所谓的 mtr 可以包含一组 redo 日志，在进行崩溃恢复时这一组 redo 日志作为一个不可分割的整体。

一个事务可以包含若干条语句，每一条语句其实是由若干个 mtr 组成，每一个 mtr 又可以包含若干条 redo 日志，画个图表示它们的关系就是这样：

![image_1d4hgjr7t4es1v2mf2b1bt51rf95b.png-27.6kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169489303b9e5c9f)

### 3、Redo 日志写入

**1.redo log block**



![image_1d4hp4u8g13e317mkngoag21clv7i.png-113.9kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169489303c1fb389)

**2.redo日志缓冲区**

写入 redo 日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为 redo log buffer 的连续内存空间（通过启动参数 innodb_log_buffer_size 修改，默认值为 16MB ），翻译成中文就是 redo日志缓冲区 ，我们也可以简称为 log buffer 。

![image_1d4jsd7861q6dn9n17gs1cdd1kek7h.png-102.6kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169489304969c9d7)

 InnoDB 定义了 buf_free 的全局变量，该变量标识当前 log buffer 空闲位置，指明后续写入的 redo 日志应该写入到 log buffer 中的哪个位置。

每个 mtr 运行过程中产生的日志先暂时存到一个地方，当该 mtr 结束的时候，将过程中产生的一组 redo 日志再全部复制到 log buffer 中。

**3.Redo 日志刷盘**

 mtr 运行过程中产生的一组 redo 日志在 mtr 结束时会被复制到 log buffer 中，在一些情况下它们会被刷新到磁盘里：

-  log buffer 空间不足时
- 事务提交时
- 后台线程不停的刷刷刷：
- 正常关闭服务器时
-  checkpoint 时

**这就是MySQL的WAL（Write-Ahead Logging）技术！**：即先写日志，再写磁盘。

### 4、Redo 日志文件

**1.Redo日志文件格式**

 redo 日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成：

- 前2048个字节，也就是前4个block是用来存储一些管理信息的。
- 从第2048字节往后是用来存储 log buffer 中的block内容的。

**2.日志文件组**

磁盘上的 redo 日志文件不只一个，而是以一个 日志文件组 的形式出现的。这些文件以 ib_logfile[数字] （ 数字 可以是 0 、 1 、 2 ...）的形式进行命名。在将 redo 日志写入 日志文件组 时，是从 ib_logfile0 开始写，如果 ib_logfile0 写满了，就接着 ib_logfilen 写。如果写到最后一个文件就重新转到 ib_logfile0 继续写。

![image_1d4njgt351je21kitk7u1gbioa46j.png-64.9kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b899033d57b7a)

**3.日志文件元数据**

**log file header**

![image_1d4nfhoa914vbne4kao7cstr95m.png-65.5kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b899065200011)

各个属性的具体释义如下：

| 属性名                 | 长度（单位：字节） | 描述                                                         |
| ---------------------- | ------------------ | ------------------------------------------------------------ |
| `LOG_HEADER_FORMAT`    | `4`                | `redo`日志的版本，在`MySQL 5.7.21`中该值永远为1              |
| `LOG_HEADER_PAD1`      | `4`                | 做字节填充用的，没什么实际意义，忽略～                       |
| `LOG_HEADER_START_LSN` | `8`                | 标记本`redo`日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值（关于什么是LSN我们稍后再看哈，看不懂的先忽略）。 |
| `LOG_HEADER_CREATOR`   | `32`               | 一个字符串，标记本`redo`日志文件的创建者是谁。正常运行时该值为`MySQL`的版本号，比如：`"MySQL 5.7.21"`，使用`mysqlbackup`命令创建的`redo`日志文件的该值为`"ibbackup"`和创建时间。 |
| `LOG_BLOCK_CHECKSUM`   | `4`                | 本block的校验值，所有block都有，我们不关心                   |

**checkpoint1/checkpoint2**：记录关于 checkpoint 的一些属性，看一下它的结构（ checkpoint2 结构和 checkpoint1 一样）：

![image_1d4njq08pd2a5j9pc01qcn2ps7g.png-60.1kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b899037defb21)

各个属性的具体释义如下：

| 属性名                        | 长度（单位：字节） | 描述                                                         |
| ----------------------------- | ------------------ | ------------------------------------------------------------ |
| `LOG_CHECKPOINT_NO`           | `8`                | 服务器做`checkpoint`的编号，每做一次`checkpoint`，该值就加1。 |
| `LOG_CHECKPOINT_LSN`          | `8`                | 服务器做`checkpoint`结束时对应的`LSN`值，系统崩溃恢复时将从该值开始。 |
| `LOG_CHECKPOINT_OFFSET`       | `8`                | 上个属性中的`LSN`值在`redo`日志文件组中的偏移量              |
| `LOG_CHECKPOINT_LOG_BUF_SIZE` | `8`                | 服务器在做`checkpoint`操作时对应的`log buffer`的大小         |
| `LOG_BLOCK_CHECKSUM`          | `4`                | 本block的校验值，所有block都有，我们不关心                   |

### 5、Checkpoint机制

**1.Log Sequence Number**

LSN：  Log Sequence Number 全局变量代表系统写入的 redo 日志量的一个总和，一个 mtr 中产生多少日志， lsn 的值就增加多少（当然有时候要加上 log block header 和 log block trailer 的大小），简称 lsn （初始值规定为 8704）。

flushed_to_disk_lsn： redo 日志是首先写到 log buffer 中，之后才会被刷新到磁盘上的 redo 日志文件， buf_next_to_write 的全局变量标记当前 log buffer 中已经有哪些日志被刷新到磁盘中了，相应的，设计 InnoDB 的大叔提出了一个表示刷新到磁盘中的 redo 日志量的全局变量，称之为 flushed_to_disk_lsn 。

flushed_to_disk_lsn 的初始值和初始的 lsn 值是相同的，都是 8704 当有新的 redo 日志写入到 log buffer 时，首先 lsn 的值会增长，但 flushed_to_disk_lsn 不变，随后随着不断有 log buffer 中的日志被刷新到磁盘上， flushed_to_disk_lsn 的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。

![image_1d4v40upc1tnt1dpe1l14u2ar4n23.png-100.2kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b899065ece690)

**2.flush 链表中的 LSN**

当第一次修改某个缓存在 Buffer Pool 中的页面时，就会把这个页面对应的控制块插入到 flush链表 的头部，之后再修改该页面时由于它已经在 flush 链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性：

-  oldest_modification ：如果某个页面被加载到 Buffer Pool 后进行第一次修改，那么就将修改该页面的 mtr 开始时对应的 lsn 值写入这个属性。
-  newest_modification ：每修改一次页面，都会将修改该页面的 mtr 结束时对应的 lsn 值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统 lsn 值。

![image_1d4v68bhl1jb9r8m6vn1b157cn5e.png-110.8kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b89909693bfe9)

flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。

**3.checkpoint LSN**

redo日志只是为了系统崩溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统崩溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。MySQL定义了 checkpoint_lsn 来代表当前系统中可以被覆盖的 redo 日志总量是多少，这个变量初始值也是 8704 。

例如，现在 页a 被刷新到了磁盘，相关的 redo 日志就可以被覆盖了，所以我们可以进行一个增加 checkpoint_lsn 的操作，我们把这个过程称之为做一次 checkpoint 。

- 步骤一：计算一下当前系统中可以被覆盖的 redo 日志对应的 lsn 值最大是多少：就是 flush 链表中最早修改的脏页（表尾节点）对应的 oldest_modification 值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的 oldest_modification 赋值给 checkpoint_lsn 。 
- 将 checkpoint_lsn 和对应的 redo 日志文件组偏移量以及此次 checkpint 的编号写到日志文件的管理信息（就是 checkpoint1 或者 checkpoint2 ）中。checkpoint 的信息只会被写到日志文件组的第一个日志文件的管理信息中。每做一次 checkpoint ，checkpoint_no 变量的值就加1， 当 checkpoint_no 的值是偶数时，就写到 checkpoint1 中，是奇数时，就写到 checkpoint2 中。

![image_1d678eiie125j1flp1tc617jp1dvo9.png-68.1kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b8990aeb41002)

 每次执行update、delete等语句更改记录时，缓冲池中的页与磁盘不一致，但是缓冲池的页不能频繁刷新到磁盘中（频率过大性能低），因此增加了write ahead log策略，当事务提交时先写重做日志，再修改内存页。当发生宕机时通过重做日志来恢复。checkpint解决以下问题：

  （1）减少重做日志大小，缩减数据恢复时间。

  （2）缓冲池不够用时将脏页刷回磁盘。

  （3）重做日志不可用时将脏页刷回磁盘（如写满）。

 show variables like 'innodb_max_dirty_pages_pct'; (默认75%)来控制inndodb强制进行checkpoint。

 

若每个重做日志大小为1G，定了了两个总共2G，则：

  **asyn_water_mark = 75 % \* 重做日志总大小。**

  **syn_water_mark = 90 % \* 重做日志总大小。**

  （1）当checkpoint_age < **asyn_water_mark**时则不需要刷新脏页回盘。

  （2）当**syn_water_mark <** checkpoint_age < **syn_water_mark** 时触发ASYNC FLUSH**。**

  （3）当checkpoint_age>**syn_water_mark**触发sync flush，此情况很少发生，一般出现在大量load data或bulk insert时。

### 6、崩溃恢复

**1.确定恢复的起点**

 checkpoint_lsn 之前的 redo 日志都可以被覆盖，也就是说这些 redo 日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。 redo 日志文件组的第一个文件的管理信息中有两个block都存储了 checkpoint_lsn 的信息，只要把 checkpoint1 和 checkpoint2 这两个block中的 checkpoint_no 值读出来比一下大小，哪个的 checkpoint_no 值更大，说明哪个block存储的就是最近的一次 checkpoint 信息，也就是我们回复的起点。

**2.确定恢复的终点**

普通block的 log block header 部分有一个称之为 LOG_BLOCK_HDR_DATA_LEN 的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为 512 。如果该属性的值不为 512 就是回复重点（LSN不就应该指向这个位置吗？）

**3.怎么恢复**

- **哈希表**：根据 redo 日志的 space ID 和 page number 属性计算出散列值，把 space ID 和 page number 相同的 redo 日志放到哈希表的同一个槽里，如果有多个 space ID 和 page number 都相同的 redo 日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的。然后遍历哈希表，因为对同一个页面进行修改的 redo 日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。如图所示：

![image_1d50lj9da176rojd12ja1lodognc.png-156.4kB](https://gitee.com/bruceyum/pictures/raw/master/pics/169b8990d0161bfc)

- **跳过已经刷新到磁盘的页面**： checkpoint_lsn 之后的 redo 日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次 checkpoint 后，可能后台线程又不断的从 LRU链表 和 flush链表 中将一些脏页刷出 Buffer Pool 。在 File Header 里有一个称之为 FIL_PAGE_LSN 的属性，该属性记载了最近一次修改页面时对应的 lsn 值（其实就是页面控制块中的 newest_modification 值）。如果在做了某次 checkpoint 之后有脏页被刷新到磁盘中，那么该页对应的 FIL_PAGE_LSN 代表的 lsn 值肯定大于 checkpoint_lsn 的值，凡是符合这种情况的页面就不需要重复执行lsn值小于 FIL_PAGE_LSN 的redo日志了。





## Binlog 日志

BinLog 是记录所有数据库表结构变更（例如 create、alter table）以及表数据修改（insert、update、delete）的二进制日志，主从数据库同步用到的都是 BinLog 文件。

BinLog 日志文件有三种模式：

**STATEMENT 模式**

- 内容：binlog 只会记录可能引起数据变更的 SQL 语句；
- 优势：该模式下因为没有记录实际的数据，所以日志量和 IO 都消耗很低，性能是最优的；
- 劣势：但有些操作并不是确定的，比如 uuid() 函数会随机产生唯一标识，当依赖 binlog 回放时，该操作生成的数据与原数据必然是不同的，此时可能造成无法预料的后果。

**ROW 模式**

- 内容：在该模式下，binlog 会记录每次操作的源数据与修改后的目标数据，StreamSets 就要求该模式；
- 优势：可以绝对精准的还原，从而保证了数据的安全与可靠，并且复制和数据恢复过程可以是并发进行的；
- 劣势：缺点在于 binlog 体积会非常大。同时，对于修改记录多、字段长度大的操作来说，记录时性能消耗会很严重。阅读的时候也需要特殊指令来进行读取数据。

**MIXED 模式**

- 内容：是对上述 STATEMENT 跟 ROW  两种模式的混合使用；
- 细节：对于绝大部分操作，都使用 STATEMENT 来进行 binlog 的记录，只有以下操作使用 ROW 来实现：表的存储引擎为 NDB，使用了uuid() 等不确定函数，使用了 insert delay 语句，使用了临时表。



![图片](https://mmbiz.qpic.cn/mmbiz_png/wJvXicD0z2dUQrRBUyxETV2RgzXuPqjscM2Fp9cfschice1hHVoTicZwJAfggMichIjt1rJnhqXSpWviaAR5BeiaaKTg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



主从同步流程：

1. 主节点必须启用二进制日志，记录任何修改了数据库数据的事件；
2. 从节点开启一个线程（I/O Thread）把自己扮演成 MySQL 的客户端，通过 MySQL 协议，请求主节点的二进制日志文件中的事件 ；
3. 主节点启动一个线程（dump Thread），检查自己二进制日志中的事件，跟对方请求的位置对比。如果不带请求位置参数，则主节点就会从第一个日志文件中的第一个事件一个一个发送给从节点；
4. 从节点接收到主节点发送过来的数据把它放置到中继日志（Relay log）文件中。并记录该次请求到主节点的具体哪一个二进制日志文件内部的哪一个位置（主节点中的二进制文件会有多个）；
5. 从节点启动另外一个线程（SQL Thread ），把 Relay log 中的事件读取出来，并在本地再执行一次。

MySQL 默认的复制方式是异步的，并且复制的时候是有并行复制能力的。主库把日志发送给从库后不管了。

这样会产生一个问题：假设主库挂了，从库处理失败了，这时候从库升为主库后，日志就丢失了。

由此产生两个概念：

**全同步复制**

主库写入 binlog 后强制同步日志到从库。所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。

**半同步复制**

半同步复制的逻辑是这样，从库写入日志成功后返回 ACK 确认给主库，主库收到至少一个从库的确认就认为写操作完成。

还可以延伸到由于主从配置不一样、主库大事务、从库压力过大、网络震荡等造成主备延迟。

如何避免这个问题？

- 主备切换的时候用可靠性优先原则还是可用性优先原则？
- 如何判断主库Crash了？
- 互为主备情况下如何避免主备循环复制？
- 被删库跑路了如何正确恢复？



## 隔离级别与MVCC

### 1、事务并发问题

在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。

**1.脏写**：如果一个事务修改了另一个未提交事务修改过的数据，那就意味着发生了 脏写 。一般在现实生活中常会遇到，例如：T<sub>1</sub> 和 T<sub>2</sub> 两个事务都对一个数据进行修改，T<sub>1</sub> 先修改并提交生效，T<sub>2</sub> 随后修改，T<sub>2</sub> 的修改覆盖了 T<sub>1</sub> 的修改。 

**2.脏读**：读脏数据指在不同的事务下，当前事务可以读到另外事务未提交的数据。例如：T<sub>1</sub> 修改一个数据但未提交，T<sub>2</sub> 随后读取这个数据。如果 T<sub>1</sub> 撤销了这次修改，那么 T<sub>2</sub> 读取的数据是脏数据。

**3.不可重复读**：不可重复读指在一个事务内多次读取同一数据集合。在这一事务还未结束前，另一事务也访问了该同一数据集合并做了修改，由于第二个事务的修改，第一次事务的两次读取的数据可能不一致。例如：T<sub>2</sub> 读取一个数据，T<sub>1</sub> 对该数据做了修改。如果 T<sub>2</sub> 再次读取这个数据，此时读取的结果和第一次读取的结果不同。

**4.幻影读**：幻读本质上也属于不可重复读的情况，T<sub>1</sub> 读取某个范围的数据，T<sub>2</sub> 在这个范围内插入新的数据，T<sub>1</sub> 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。

并发一致性问题严重性排序：脏写 > 脏读 > 不可重复读 > 幻读。

产生并发不一致性问题的主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。

### 2、事务隔离级别

**未提交读（READ UNCOMMITTED）**：事务中的修改，即使没有提交，对其它事务也是可见的。

**提交读（READ COMMITTED）**：一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。

**可重复读（REPEATABLE READ）**：保证在同一个事务中多次读取同一数据的结果是一样的。

**可串行化（SERIALIZABLE）**：强制事务串行执行，这样多个事务互不干扰，不会出现并发一致性问题。该隔离级别需要加锁实现，因为要使用加锁机制保证同一时间只有一个事务执行，也就是保证事务串行执行。

| 隔离级别           | 脏读         | 不可重复读   | 幻读         |
| ------------------ | ------------ | ------------ | ------------ |
| `READ UNCOMMITTED` | Possible     | Possible     | Possible     |
| `READ COMMITTED`   | Not Possible | Possible     | Possible     |
| `REPEATABLE READ`  | Not Possible | Not Possible | Possible     |
| `SERIALIZABLE`     | Not Possible | Not Possible | Not Possible |

注：脏写这个问题太严重了，不论是哪种隔离级别，都不允许脏写的情况发生。

### 3、MVCC原理

多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。

**1.基本思想**

加锁能解决多个事务同时执行时出现的并发一致性问题、，在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。

在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照（Undo 日志）。

脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。

**2.版本链**

对于使用 InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列

-  trx_id ：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的 事务id 赋值给 trx_id 隐藏列。
-  roll_pointer ：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo日志 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。

每次对记录进行改动，都会记录一条 undo日志 ，每条 undo日志 也都有一个 roll_pointer 属性，所有的版本都会被 roll_pointer 属性连接成一个链表，我们把这个链表称之为 版本链 ，版本链的头节点就是当前记录最新的值。

![image_1d8po6kgkejilj2g4t3t81evm20.png-81.7kB](https://gitee.com/bruceyum/pictures/raw/master/pics/16a33e277a98dbec)

**3.ReadView**

对于使用 READ UNCOMMITTED 隔离级别的事务来说，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了；对于使用 SERIALIZABLE 隔离级别的事务来说，MySQL 规定使用加锁的方式来访问记录（加锁是啥我们后续文章中说哈）；对于使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是：需要判断一下版本链中的哪个版本是当前事务可见的。为此，MySQL 提出了一个 ReadView 的概念，这个 ReadView 中主要包含4个比较重要的内容：

-  m_ids ：表示在生成 ReadView 时当前系统中活跃的读写事务的 事务id 列表。
-  min_trx_id ：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的 事务id ，也就是 m_ids 中的最小值。
-  max_trx_id ：表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。
-  creator_trx_id ：表示生成该 ReadView 的事务的 事务id 。

有了这个 ReadView ，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：

- 如果被访问版本的 trx_id 属性值与 ReadView 中的 creator_trx_id 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。
- 如果被访问版本的 trx_id 属性值小于 ReadView 中的 min_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。
- 如果被访问版本的 trx_id 属性值大于或等于 ReadView 中的 max_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。
- 如果被访问版本的 trx_id 属性值在 ReadView 的 min_trx_id 和 max_trx_id 之间，那就需要判断一下 trx_id 属性值是不是在 m_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。

如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录。

在 MySQL 中， READ COMMITTED 和 REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同。

**4.READ COMMITTED —— 每次读取数据前都生成一个ReadView**

存在3个事务，trx_id = 100， trx_id = 200 以及trx_id = 0的读事务：

tri_id = 100的事务对number = 1的记录进行了两次修改，刘备->关羽->张飞，此时trx_id=0 的读事务去读取 number = 1的记录：

![image_1d8poeb056ck1d552it4t91aro2d.png-63.7kB](https://gitee.com/bruceyum/pictures/raw/master/pics/16a33e277e11d7b8)

- 在执行 SELECT 语句时会先生成一个 ReadView ， ReadView 的 m_ids 列表的内容就是 [100, 200] ， min_trx_id 为 100 ， max_trx_id 为 201 ， creator_trx_id 为 0 。
- 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 '张飞' ，该版本的 trx_id 值为 100 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。直到 trx_id = 80的记录小于 min_trix_id 服务和可见性要求，最后返回给用户的版本就是这条列 name 为 '刘备' 的记录。

接着trx_id = 100的事务提交，然后trx_id = 200的事务对 number = 1 的记录进行修改，张飞-> 赵云->诸葛亮，然后trx_id = 0的读事务读取 number = 1的记录：

![image_1d8poudrjdrk4k0i22bj10g82q.png-78.6kB](https://gitee.com/bruceyum/pictures/raw/master/pics/16a33e277f08dc3c)

- 在执行 SELECT 语句时会又会单独生成一个 ReadView ，该 ReadView 的 m_ids 列表的内容就是 [200] ， min_trx_id 为 200 ， max_trx_id 为 201 ， creator_trx_id 为 0 。
- 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 '诸葛亮' ，该版本的 trx_id 值为 200 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。直到 trx_id = 100的第一条记录，符合可见性要求，最后返回给用户的版本就是这条列 name 为 '张飞' 的记录。出现不可重复读问题。

由此可见 READ COMMITTED隔离级别，通过每次SELECT 都创建一个ReadView实现读已提交，并且会出现不可重复读问题。

**5.REPEATABLE READ —— 在第一次读取数据时生成一个ReadView**

还是上面的例子，但是REPEATABLE READ只在第一次读取数据时生成一个ReadView，第二次读取 number = 1的记录：

- 在第一次执行 SELECT 语句时会先生成一个 ReadView ， ReadView 的 m_ids 列表的内容就是 [100, 200] ， min_trx_id 为 100 ， max_trx_id 为 201 ， creator_trx_id 为 0 。

- 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 '诸葛亮' ，该版本的 trx_id 值为 200 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。直到 trx_id = 80 的记录小于 min_trix_id 服务和可见性要求，最后返回给用户的版本就是这条列 name 为 '刘备' 的记录。

由此可见REPEATABLE READ 隔离级别，通过在第一次读取数据时生成一个ReadView实现读已提交，并且会出现不可重复读问题。

 MVCC （Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行普通的 SELECT 操作时访问记录的版本链的过程，这样子可以使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。 READ COMMITTD 、 REPEATABLE READ 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了。

### 4、两阶段提交





## 锁

### 1、封锁粒度

MySQL 中提供了两种封锁粒度：行级锁以及表级锁。

应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。

但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。

在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。


### 2、封锁类型

**1.读写锁**

- 互斥锁（Exclusive），简写为 X 锁，又称写锁。
- 共享锁（Shared），简写为 S 锁，又称读锁。

有以下两个规定：

- 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。
- 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。

锁的兼容关系如下：

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191207213523777.png"/> </div><br>

**2.意向锁**

使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。

在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。

意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定：

- 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁；
- 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。

通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。

各种锁的兼容关系如下：

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191207214442687.png"/> </div><br>

解释如下：

- 任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁；
- 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T<sub>1</sub> 想要对数据行 R<sub>1</sub> 加 X 锁，事务 T<sub>2</sub> 想要对同一个表的数据行 R<sub>2</sub> 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。）

**3.AUTO-INC锁**

在使用 MySQL 过程中，我们可以为表的某个列添加 AUTO_INCREMENT 属性，之后在插入记录时，可以不指定该列的值，系统会自动为它赋上递增的值。系统实现这种自动给 AUTO_INCREMENT 修饰的列递增赋值的原理主要是两个：

- 采用 AUTO-INC 锁，也就是在执行插入语句时就在表级别加一个 AUTO-INC 锁，然后为每条待插入记录的 AUTO_INCREMENT 修饰的列分配递增的值，在该语句执行结束后，再把 AUTO-INC 锁释放掉。这样一个事务在持有 AUTO-INC 锁的过程中，其他事务的插入语句都要被阻塞，可以保证一个语句中分配的递增值是连续的。
- 采用一个轻量级的锁，在为插入语句生成 AUTO_INCREMENT 修饰的列的值时获取一下这个轻量级锁，然后生成本次插入语句需要用到的 AUTO_INCREMENT 列的值之后，就把该轻量级锁释放掉，并不需要等到整个插入语句执行完才释放锁。



### 3、封锁协议

**1.一级封锁协议**  

事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。

可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191207220440451.png" width=600/> </div><br>

**2.二级封锁协议**  

在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。

可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191207220831843.png" width=600/> </div><br>

**3.三级封锁协议**  

在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。

可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191207221313819.png" width=600/> </div><br>

**4.两段锁协议**

加锁和解锁分为两个阶段进行。

可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。串行执行的事务互不干扰，不会出现并发一致性问题。

事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。

```html
lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B)
```

但不是必要条件，例如以下操作不满足两段锁协议，但它还是可串行化调度。

```html
lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C)
```

### 4、隐式与显示锁定

MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。

InnoDB 也可以使用特定的语句进行显示锁定：

```sql
SELECT ... LOCK In SHARE MODE;
SELECT ... FOR UPDATE;
```

### 5、InnoDB中的行级锁

**1.Record Locks**

锁定一个记录上的索引，而不是记录本身，类型名称为： LOCK_REC_NOT_GAP 

 记录锁 是有 S锁 和 X锁 之分的，让我们分别称之为 S型记录锁 和 X型记录锁 吧（听起来有点怪怪的），当一个事务获取了一条记录的 S型记录锁 后，其他事务也可以继续获取该记录的 S型记录锁 ，但不可以继续获取 X型记录锁 ；当一个事务获取了一条记录的 X型记录锁 后，其他事务既不可以继续获取该记录的 S型记录锁 ，也不可以继续获取 X型记录锁 ；

**2.Gap Locks**

 MySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方案解决，也可以采用 加锁 方案解决。但是在使用 加锁 方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上 记录锁 。

MyQSLR 定义了  LOCK_GAP 锁，我们也可以简称为 gap锁 ，锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。

```sql
SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;
```

 gap锁 的提出仅仅是为了防止插入幻影记录而提出的，虽然有 共享gap锁 和 独占gap锁 这样的说法，但是它们起到的作用都是相同的。而且如果你对一条记录加了 gap锁 （不论是 共享gap锁 还是 独占gap锁 ），并不会限制其他事务对这条记录加 正经记录锁 或者继续加 gap锁 

**3.Next-Key Locks**

Next-Key Locks 是 MySQL 的 InnoDB 存储引擎的一种锁实现，类型名称为： LOCK_ORDINARY ，我们也可以简称为 next-key锁 

MVCC 不能解决幻影读问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。

它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间：

```sql
(-∞, 10]，(10, 11]，(11, 13]，(13, 20]，(20, +∞)
```

**4.Insert Intention Locks**







## 参考资料

- AbrahamSilberschatz, HenryF.Korth, S.Sudarshan, 等. 数据库系统概念 [M]. 机械工业出版社, 2006.
- 施瓦茨. 高性能 MYSQL(第3版)[M]. 电子工业出版社, 2013.
- 史嘉权. 数据库系统概论[M]. 清华大学出版社有限公司, 2006.
- [The InnoDB Storage Engine](https://dev.mysql.com/doc/refman/5.7/en/innodb-storage-engine.html)
- [Transaction isolation levels](https://www.slideshare.net/ErnestoHernandezRodriguez/transaction-isolation-levels)
- [Concurrency Control](http://scanftree.com/dbms/2-phase-locking-protocol)
- [The Nightmare of Locking, Blocking and Isolation Levels!](https://www.slideshare.net/brshristov/the-nightmare-of-locking-blocking-and-isolation-levels-46391666)
- [Database Normalization and Normal Forms with an Example](https://aksakalli.github.io/2012/03/12/database-normalization-and-normal-forms-with-an-example.html)
- [The basics of the InnoDB undo logging and history system](https://blog.jcole.us/2014/04/16/the-basics-of-the-innodb-undo-logging-and-history-system/)
- [MySQL locking for the busy web developer](https://www.brightbox.com/blog/2013/10/31/on-mysql-locks/)
- [浅入浅出 MySQL 和 InnoDB](https://draveness.me/mysql-innodb)
- [Innodb 中的事务隔离级别和锁的关系](https://tech.meituan.com/2014/08/20/innodb-lock.html)
